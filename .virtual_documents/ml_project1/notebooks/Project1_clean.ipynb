


import autograd.numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
sns.set_style("darkgrid", {"axes.facecolor": ".95"})
import sys
import random
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from autograd import grad

# own functions
from ml_project1.functions import *


from matplotlib import rcParams
rcParams['text.usetex'] = True
rcParams['text.latex.preamble'] = r'\usepackage{sansmathfonts}'
rcParams.update({
    "font.size": 10,       
    "axes.labelsize": 10,
    "xtick.labelsize": 10,
    "ytick.labelsize": 10,
})
def saveplot(plotname):
    return plt.savefig("LaTeX/images/{}.pdf".format(str(plotname)))
cw = 8.325/2.54
tw = 17/2.54








# defining inputs, train_test splitting
n = 100
p_range = np.arange(1, 29, 1)

x = np.linspace(-1, 1, n)
y = Runge(x, noise=False)
y_noise = Runge(x, noisescale=0.1)

y_centered = y - np.mean(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train.reshape(-1, 1)).flatten()
x_test = scaler.transform(x_test.reshape(-1, 1)).flatten()
x = scaler.transform(x.reshape(-1, 1)).flatten()


# plot Runge 
plt.figure(figsize=(cw, 8/2.54))
plt.plot(x, y_noise, label=r'With noise $\epsilon \in N(0,0.1)$', alpha=0.6, color="darkorange")
plt.plot(x, y, label=r"$y = \mathrm{Runge}(x)$", color="royalblue")
plt.xlabel(r"$x$ values")
plt.ylabel(r"$y$ values")
plt.tight_layout()
saveplot('runge')





lr = LinearRegression_own(intercept=True)

# design matrices
X_train_array = [lr.polynomial_features(x_train, p) for p in p_range]
X_test_array = [lr.polynomial_features(x_test, p) for p in p_range]

# fit
theta_OLS_array = [lr.fit(X_train, y_train, method='OLS') for X_train in X_train_array]

# prediction
y_pred_train_array = [lr.predict(X_train, theta) for X_train, theta in zip(X_train_array, theta_OLS_array)]
y_pred_test_array = [lr.predict(X_test, theta) for X_test, theta in zip(X_test_array, theta_OLS_array)]


j = 9
deg = j
cmap = sns.color_palette("Paired")


plt.figure(figsize=(14/2.54, 9/2.54))
plt.plot(x, y_noise, label=r"$y = \mathrm{Runge}(x) + \epsilon$", alpha=0.7)
plt.scatter(x_train, y_pred_train_array[j], color=cmap[6], label=f"Predicted train values")
plt.scatter(x_test, y_pred_test_array[j], color=cmap[7], label=f"Predicted test values")
plt.title(f"Ordinary Least Squares regression, degree = {deg}")
plt.xlabel(r"Scaled $x$ values")
plt.ylabel(r"$y$ values")
plt.legend()
plt.tight_layout()
saveplot('polyfit')


# fit
theta_Ridge_array = [lr.fit(X_train, y_train, method='Ridge') for X_train in X_train_array]

# prediction
y_pred_train_array = [lr.predict(X_train, theta) for X_train, theta in zip(X_train_array, theta_OLS_array)]
y_pred_test_array = [lr.predict(X_test, theta) for X_test, theta in zip(X_test_array, theta_OLS_array)]








MSE_train_array = [mean_squared_error(y_train, y_pred_train) for y_pred_train in y_pred_train_array]
MSE_test_array = [mean_squared_error(y_test, y_pred_test) for y_pred_test in y_pred_test_array]
r2_train_array = [r2_score(y_train, y_pred_train) for y_pred_train in y_pred_train_array]
r2_test_array = [r2_score(y_test, y_pred_test) for y_pred_test in y_pred_test_array]


fig, axes = plt.subplots(1, 2, figsize=(17/2.54, 8/2.54))
ax0 = axes[0]
ax1 = axes[1]

ax0.plot(p_range, MSE_train_array, color=cmap[6], label=r"MSE on train set")
ax0.plot(p_range, MSE_test_array, color=cmap[7], label=r"MSE on test set")
ax0.set_xlabel(r"Polynomial degree $p$")
ax0.set_ylabel(r"Mean Squared Error MSE")
ax0.legend()

ax1.plot(p_range, r2_train_array, color=cmap[8], label=r"$R^2$ on train set")
ax1.plot(p_range, r2_test_array, color=cmap[9], label=r"$R^2$ on test set")
ax1.set_xlabel(r"Polynomial degree $p$")
ax1.set_ylabel(r"Determination coefficient $R^2$")
ax1.legend()

plt.tight_layout()
saveplot('MSE_R2_p1')





MSE_train_matrix, MSE_test_matrix, Rsquared_train_matrix, Rsquared_test_matrix = MSE_R2_pn()


n_range = np.logspace(1, 5, 10).astype(int)


matrixes = [MSE_test_matrix, Rsquared_test_matrix]
titles = ["MSE on test set", r"$R^2$ on test set"]
cmaps = [sns.color_palette("YlOrBr_r", as_cmap=True), sns.color_palette("viridis_r", as_cmap=True)]

fig, axes = plt.subplots(1, 2, figsize=(17/2.54, 8/2.54), sharey=True)

for i, ax in enumerate(axes.flat):
    sns.heatmap(matrixes[i],
                ax=ax,
                cmap=cmaps[i],
                cbar_kws={"shrink": 0.7},)
    ax.set_title(titles[i])
    ax.set_xlabel(r"Polynomial degree $p$")
    ax.set_yticks(np.arange(len(matrixes[i])) + 0.5)

axes[0].set_ylabel(r"Dimensionality $n$")
axes[0].set_yticklabels([f"{n:.0e}" for n in n_range], rotation=0)
plt.tight_layout()
saveplot('MSE_R2_OLS_test')
plt.show()









lbdas = np.logspace(-4, 3, 8)
X_trainR = X_train_array[j]
X_testR = X_test_array[j]

cmapr = sns.color_palette(palette='magma', n_colors=len(lbdas))

plt.figure(figsize=(17/2.54, 10/2.54))
plt.plot(x, y_noise, alpha=0.7)


# fit
for i, lbda in enumerate(lbdas):
    beta_R = lr.fit(X_trainR, y_train, method='Ridge', lbda=lbda)
    y_pred_trainR = lr.predict(X_trainR, beta_R)
    y_pred_testR = lr.predict(X_testR, beta_R)
    plt.scatter(x_train, y_pred_trainR, color=cmapr[i], label=rf"{lbda:.0e}", marker='.')
    plt.scatter(x_test, y_pred_testR, color=cmapr[i], marker='.')
    plt.xlabel(r"Scaled $x$ values")

plt.title(rf"Ridge regression for $p$ = {deg} and different $\lambda$")
plt.ylabel(r"$y$ values")
plt.legend(title=r"$\lambda$")
plt.tight_layout()
saveplot('ridge')





beta_ridge_array = []
lbdas2 = np.logspace(-4, 3, 20)

for X_train in X_train_array:
    beta_for_X = [lr.fit(X_train, y_train, method='Ridge', lbda=lbda) for lbda in lbdas2]
    beta_ridge_array.append(beta_for_X)



MSE_test_matrix_ridge = np.zeros((len(lbdas2), len(p_range)))      # rows = lambda, cols = p
Rsquared_test_matrix_ridge = np.zeros_like(MSE_test_matrix_ridge)

for i, lbda in enumerate(lbdas2):
    for j, p in enumerate(p_range):
       
        # fit
        X_train_poly = lr.polynomial_features(x_train, p)
        X_test_poly  = lr.polynomial_features(x_test, p)

        beta = lr.fit(X_train_poly, y_train, method='Ridge', lbda=lbda)

        y_pred_test = lr.predict(X_test_poly, beta)

        # update estimators matrices
        MSE_test_matrix_ridge[i, j] = mean_squared_error(y_test, y_pred_test)
        Rsquared_test_matrix_ridge[i, j] = r2_score(y_test, y_pred_test)



matrixes = [MSE_test_matrix_ridge, Rsquared_test_matrix_ridge]
titles = ["MSE on test set", r"$R^2$ on test set"]
cmaps = [sns.color_palette("viridis", as_cmap=True), sns.color_palette("Spectral", as_cmap=True)]

fig, axes = plt.subplots(1, 2, figsize=(17/2.54, 8/2.54), sharey=True)

n = 3  # show every 2nd lambda
yticks_idx = np.arange(1, len(lbdas2), n)
yticks_labels = [f"{lbdas2[i]:.0e}" for i in yticks_idx]


for i, ax in enumerate(axes.flat):
    sns.heatmap(matrixes[i],
                ax=ax,
                cmap=cmaps[i],
                cbar_kws={"shrink": 0.7})
    ax.set_title(titles[i])
    ax.set_xlabel(r"Polynomial degree $p$")

axes[0].set_ylabel(r"Ridge penalty $\lambda$")
axes[0].set_yticks(yticks_idx)
axes[0].set_yticklabels(yticks_labels, rotation=0)
plt.tight_layout()
saveplot('MSE_R2_Ridge_p_lambda1')
plt.show()











def theta_gd(X, y, eta, regression_method='OLS', lbda=0.1, iterations=200000, converge=1e-8):
    """ Computes optimal parameters for ordinary least squares regression with gradient descent.
        Parameters:
        :: X (matrix) = design matrix obtained with polynomial_features(x, p, intercept)
        :: y (array) = true value to be modeled
        :: eta (scalar) = learning rate
        :: regression_method (str) = 'OLS' or 'Ridge'
        :: lbda (scalar) = regularization parameter (only for Ridge)
        :: iterations (int) = maximum number of iterations
        :: converge (scalar) = convergence criterion
    """
    # error for wrong regression_method input
    if regression_method not in ['OLS', 'Ridge']:
        raise ValueError("regression_method must be 'OLS' or 'Ridge'")

    # initialize theta, from y shape
    n, m = X.shape
    y = np.asarray(y).reshape(-1, 1)
    theta = np.zeros((m, 1), dtype=float)

    for k in range(iterations):
        gradient = grad_analytic(X, y, theta, lbda=lbda, regression_method=regression_method)  # compute gradient with autograd
        theta -= eta * gradient

        grad_norm = np.linalg.norm(gradient)
        if grad_norm > 1e2:  # cap gradient norm (DeepSeek hint)
            gradient = gradient / grad_norm * 1e2

        if grad_norm <= converge:
            print(f"Stop at epsilon = {grad_norm:.2e}, iteration = {k}")
            break

    return theta


p2 = 6
eta = 1e-3
iterations = 200000

X_test = X_test_array[p2] 
X_train = X_train_array[p2]
theta_gdOLS = theta_gd(X_train, y_train, eta=eta, iterations = iterations)
theta_gdRidge = theta_gd(X_train, y_train, eta=eta, iterations = iterations, regression_method='Ridge')

print(f"Testing degree {p2}")
print(f"X_train shape: {X_train.shape}")
print(f"OLS GD parameters: {theta_gdOLS}")
print(f"Ridge GD parameters: {theta_gdRidge}")
print(f"Analytical parameters: {theta_OLS_array[p2]}")


plt.figure(figsize=(12/2.54, 8/2.54))
plt.plot(x, y_noise, linewidth=1.2, label='True function')

# colors - nice DeepSeek hint for picking directly from colorspace 
c_gd_ols   = "#DE9B17"
c_gd_ridge = "#C5615C"
c_ols      = "#564e9f"

# GD OLS
plt.scatter(x_train, X_train @ theta_gdOLS, color=c_gd_ols, marker='.', s=15)
plt.scatter(x_test,  X_test  @ theta_gdOLS, color=c_gd_ols, marker='.', s=15, label='GD OLS')

# GD Ridge
plt.scatter(x_train, X_train @ theta_gdRidge, color=c_gd_ridge, marker='.', s=15)
plt.scatter(x_test,  X_test  @ theta_gdRidge, color=c_gd_ridge, marker='.', s=15, label=rf'GD Ridge, $\lambda$=0.1')

# analytical OLS
plt.scatter(x_train, X_train @ theta_OLS_array[p2], color=c_ols, marker='.', s=15, alpha=0.7)
plt.scatter(x_test,  X_test  @ theta_OLS_array[p2], color=c_ols, marker='.', s=15, label='Analytical OLS', alpha=0.7)

plt.xlabel(r"$x$")
plt.ylabel(r"$y$")
plt.title(rf"GD vs Analytical OLS/Ridge, $p = {p2}$")
plt.legend(fontsize=9)
plt.tight_layout()
saveplot('gd_analytical')
plt.show()







p_range = np.arange(1, 8, 1)
X_train_array = [lr.polynomial_features(x_train, p) for p in p_range]
X_test_array = [lr.polynomial_features(x_test, p) for p in p_range]

etas = [1e-3, 5e-4, 1e-4]
iterations_range = np.logspace(3, 6, 6, dtype=int)
p_max = len(p_range)

# init matrices
MSE_gd_1e2, Rsq_gd_1e2 = np.zeros((len(iterations_range), p_max)), np.zeros((len(iterations_range), p_max))
MSE_gd_1e3, Rsq_gd_1e3 = np.zeros((len(iterations_range), p_max)), np.zeros((len(iterations_range), p_max))
MSE_gd_1e4, Rsq_gd_1e4 = np.zeros((len(iterations_range), p_max)), np.zeros((len(iterations_range), p_max))

for eta in etas:
    MSE_mat = np.zeros((len(iterations_range), p_max))
    R2_mat  = np.zeros((len(iterations_range), p_max))

    for i, iters in enumerate(iterations_range):
        for j, p in enumerate(p_range):
            X_train_p = X_train_array[j]
            X_test_p  = X_test_array[j]

            # fit & predict
            theta = theta_gd(X_train_p, y_train, eta=eta, iterations=iters, regression_method='OLS')
            y_pred_train = X_train_p @ theta
            y_pred_test  = X_test_p @ theta

            # update estimators
            MSE_mat[i, j] = np.mean( (y_test - y_pred_test)**2 )
            R2_mat[i, j]  = 1 - np.sum( (y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2 )
    if eta == 1e-2:
        MSE_gd_1e2, Rsq_gd_1e2 = MSE_mat, R2_mat
    elif eta == 1e-3:
        MSE_gd_1e3, Rsq_gd_1e3 = MSE_mat, R2_mat
    elif eta == 1e-4:
        MSE_gd_1e4, Rsq_gd_1e4 = MSE_mat, R2_mat


eta_labels = ['10^{-3}', '5\cdot10^{-4}', '10^{-4}']
MSE_matrices = [MSE_gd_1e2, MSE_gd_1e3, MSE_gd_1e4]
R2_matrices = [Rsq_gd_1e2, Rsq_gd_1e3, Rsq_gd_1e4]

for i in range(3):
    max_mse = np.nanmax(MSE_matrices[i])
    MSE_matrices[i] = np.nan_to_num(MSE_matrices[i], nan=max_mse*10, posinf=max_mse*10)
    R2_matrices[i] = np.nan_to_num(R2_matrices[i], nan=0.0, posinf=1.0)

fig, axes = plt.subplots(2, 3, figsize=(17/2.54, 10/2.54), sharey=True)
cbar_kws = {"shrink": 0.7, "aspect": 20}
iter_labels = [f"{n:.0e}" for n in iterations_range]

for i in range(3):
    sns.heatmap(MSE_matrices[i], ax=axes[0,i], cmap='viridis', cbar_kws=cbar_kws)
    axes[0,i].set_title(f'$\eta = {eta_labels[i]}$ | MSE')
    axes[0,i].set_ylabel('Iterations')
    axes[0,i].set_yticklabels(iter_labels, rotation=0)

    sns.heatmap(R2_matrices[i], ax=axes[1,i], cmap='Spectral', cbar_kws=cbar_kws)
    axes[1,i].set_title(f'$\eta = {eta_labels[i]}$ | $R^2$')
    axes[1,i].set_xlabel('Polynomial degree $p$')
    axes[1,i].set_ylabel('Iterations')
    axes[1,i].set_yticklabels(iter_labels, rotation=0)

plt.tight_layout()
saveplot('MSE_R2_GD_eta')
plt.show()








optimizer = 'AdaGrad'
eta = 5e-4
iterations_range = np.logspace(5, 5, 3, dtype=int)
p_range = np.arange(1, 8, 1)  # polynomial degrees

MSE_mat = np.zeros((len(iterations_range), len(p_range)))
R2_mat  = np.zeros((len(iterations_range), len(p_range)))

for i, iters in enumerate(iterations_range):
    for j, p in enumerate(p_range):
        X_train_p = X_train_array[j]
        X_test_p  = X_test_array[j]

        theta = theta_gd_mom(X_train_p, y_train, eta=eta, iterations=iters, 
                              regression_method='OLS', eta_update_method=optimizer)

        y_pred_test = X_test_p @ theta

        #mse = np.mean((y_test - y_pred_test)**2)
        mse = mean_squared_error(y_test, y_pred_test)
        #r2  = 1 - np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2)
        r2 = r2_score(y_test, y_pred_test)

        MSE_mat[i, j] = np.nan_to_num(mse, nan=1e10, posinf=1e10)
        R2_mat[i, j]  = np.nan_to_num(r2,  nan=0.0, posinf=1.0)

# plot
cmaps = [sns.color_palette("viridis", as_cmap=True), sns.color_palette("Spectral", as_cmap=True)]
titles = [rf"{optimizer} | MSE", rf"{optimizer} | $R^2$"]

fig, axes = plt.subplots(1, 2, figsize=(17/2.54, 8/2.54), sharey=True)

# MSE
MSE_mat = np.clip(MSE_mat, 0, np.nanpercentile(MSE_mat, 99))
R2_mat  = np.clip(R2_mat, 0, 1)

axes[0].set_title(titles[0])
axes[0].set_xlabel(r"Polynomial degree $p$")
axes[0].set_ylabel("Iterations")
axes[0].set_yticklabels([f"{n:.0e}" for n in iterations_range], rotation=0)

# R2
sns.heatmap(R2_mat, ax=axes[1], cmap=cmaps[1], center=0, cbar_kws={"shrink":0.7})
axes[1].set_title(titles[1])
axes[1].set_xlabel(r"Polynomial degree $p$")
axes[1].set_ylabel("Iterations")
axes[1].set_yticklabels([f"{n:.0e}" for n in iterations_range], rotation=0)

plt.tight_layout()
saveplot(f"MSE_R2_{optimizer}")
plt.show()





eta_labels = ['10^{-3}', '10^{-4}', '10^{-5}']
MSE_matrices = [MSE_gd_1e2, MSE_gd_1e3, MSE_gd_1e4]
R2_matrices = [Rsq_gd_1e2, Rsq_gd_1e3, Rsq_gd_1e4]

for i in range(3):
    max_mse = np.nanmax(MSE_matrices[i])
    MSE_matrices[i] = np.nan_to_num(MSE_matrices[i], nan=max_mse*10, posinf=max_mse*10)
    R2_matrices[i] = np.nan_to_num(R2_matrices[i], nan=0.0, posinf=1.0)

fig, axes = plt.subplots(2, 3, figsize=(17/2.54, 10/2.54), sharey=True)
cbar_kws = {"shrink": 0.7, "aspect": 20}
iter_labels = [f"{n:.0e}" for n in iterations_range]

for i in range(3):
    sns.heatmap(MSE_matrices[i], ax=axes[0,i], cmap='viridis', cbar_kws=cbar_kws)
    axes[0,i].set_title(f'$\eta = {eta_labels[i]}$ | MSE')
    axes[0,i].set_ylabel('Iterations')
    axes[0,i].set_yticklabels(iter_labels, rotation=0)

    sns.heatmap(R2_matrices[i], ax=axes[1,i], cmap='Spectral', cbar_kws=cbar_kws)
    axes[1,i].set_title(f'$\eta = {eta_labels[i]}$ | $R^2$')
    axes[1,i].set_xlabel('Polynomial degree $p$')
    axes[1,i].set_ylabel('Iterations')
    axes[1,i].set_yticklabels(iter_labels, rotation=0)

plt.tight_layout()
saveplot('MSE_R2_GD_eta')
plt.show()








lbdas = np.logspace(-8, 1, 8)
X_trainR = X_train_array[j]
X_testR = X_test_array[j]

cmapr = sns.color_palette(palette='', n_colors=len(lbdas))

plt.figure(figsize=(17/2.54, 10/2.54))
plt.plot(x, y_noise, alpha=0.7)


# fit
for i, lbda in enumerate(lbdas):
    theta_Lasso = Lasso_params(X_train, y_train, lbda=lbda, iterations=100000, eta=1e-4, converge=1e-8)
    y_pred_trainL = X_train @ theta_Lasso
    y_pred_testL = X_test @ theta_Lasso
    plt.scatter(x_train, y_pred_trainL, color=cmapr[i], label=rf"{lbda:.0e}", marker='.')
    plt.scatter(x_test, y_pred_testL, color=cmapr[i], marker='.')
    plt.xlabel(r"Scaled $x$ values")

plt.title(rf"Ridge regression for $p$ = {deg} and different $\lambda$")
plt.ylabel(r"$y$ values")
plt.legend(title=r"$\lambda$")
plt.tight_layout()
saveplot('lasso')











seed = random.randrange(sys.maxsize)
rng = random.Random(seed)
print("Seed was:", seed)

n = 200
x = np.linspace(-1, 1, n)
y = Runge(x, noise=True, noisescale=0.1)

pmax = 21

degs = np.arange(0, pmax, 1)

bvt1 = BiasVarianceTradeoff(x, y, p_range=degs, n_bootstraps=100)
bvt1.degree_range_simul()
bvt1.visualize()   

saveplot('BVtradeoff_b')





fold_range = np.arange(10, 200, 20)

cv_kfold_array = []

for i, fold in enumerate(fold_range):
    cv_kfold = k_fold_cv(x, y, p_range=degs, regression_method='OLS', k=fold)
    cv_kfold_array.append(cv_kfold)


from matplotlib.cm import ScalarMappable
from matplotlib.colors import Normalize

fig, ax = plt.subplots(figsize=(14/2.54, 10/2.54))

cmap = sns.color_palette('rocket_r', n_colors=len(fold_range))
norm = Normalize(vmin=min(fold_range), vmax=max(fold_range))
sm = ScalarMappable(cmap='rocket_r', norm=norm)

for i, fold in enumerate(fold_range):
    ax.plot(degs, cv_kfold_array[i], color=cmap[i])

# ChatGPT suggestion
cv_kfold_array = np.array(cv_kfold_array)
opt_k_ind, opt_deg_ind = np.unravel_index(np.argmin(cv_kfold_array), cv_kfold_array.shape)
opt_degree = degs[opt_deg_ind]
ax.axvline(x=opt_degree, linestyle='dashed', color='gray', alpha=0.7, label=f"Optimal degree = {opt_degree}\n MSE = {np.argmin(cv_kfold_array):.2f}")
ax.legend()

ax.set_xlabel('Polynomial Degree')
ax.set_ylabel('Error')
ax.set_yscale('log')
ax.set_title("MSE estimation with $k$-fold cross validation")
ax.grid(alpha=0.3)

cbar = fig.colorbar(sm, ax=ax, ticks=fold_range)
cbar.set_label("$k$ (number of folds)")
cbar.ax.set_yticklabels([str(k) for k in fold_range])

plt.tight_layout()
saveplot('cv_bv')



