@book{bishop2006_pattern,
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  author = {Bishop, Christopher M.},
  date = {2006},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  isbn = {0-387-31073-8}
}

@inproceedings{bridle1990_softmax,
  title = {Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition},
  booktitle = {Neurocomputing: {{Algorithms}}, Architectures and Applications},
  author = {Bridle, John S.},
  date = {1990},
  pages = {227--236},
  publisher = {Springer}
}

@article{clevert2015_elu,
  title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus)},
  author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  date = {2015},
  journaltitle = {arXiv preprint},
  volume = {arXiv:1511.07289}
}

@online{CompPhysicsML,
  title = {Machine Learning and Data Analysis for the Physical Sciences},
  author = {Hjorth-Jensen, Morten},
  date = {2023},
  url = {https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week35.html},
  urldate = {2024-01-20}
}

@article{cybenko1989_approx,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12-01},
  journaltitle = {Mathematics of Control, Signals and Systems},
  shortjournal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  url = {https://doi.org/10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}
}

@article{deridder1999_nn,
  title = {The {{Applicability}} of {{Neural Networks}} to {{Non-linear Image Processing}}},
  author = {family=Ridder, given=D., prefix=de, useprefix=true and Duin, R.P.W. and Verbeek, P.W. and family=Vliet, given=L.J., prefix=van, useprefix=true},
  date = {1999-06-01},
  journaltitle = {Pattern Analysis \& Applications},
  shortjournal = {Pattern Analysis \& Applications},
  volume = {2},
  number = {2},
  pages = {111--128},
  issn = {1433-7541},
  doi = {10.1007/s100440050022},
  url = {https://doi.org/10.1007/s100440050022},
  abstract = {In this paper, the applicability of neural networks to non-linear image processing problems is studied. As an example, the Kuwahara filtering for edge-preserving smoothing was chosen. This filter is interesting due to its non-linear nature and natural modularity. A number of modular networks were constructed and trained, incorporating prior knowledge in various degrees and their performance was compared to standard feed-forward neural networks (MLPs). Based on results obtained in these experiments, it is shown that several key factors influence neural network behaviour in this kind of task. First, it is demonstrated that the mean squared error criterion used in neural network training is not representative for the problem. To be able to discern performance differences better, a new error measure for edge-preserving smoothing operations is proposed. Secondly, using this measure, it is shown that modular networks perform better than standard feed-forward networks. The latter type often ends up in linear approximations to the filter. Finally, inspection of the modular networks shows that, although analysis is difficult due to their non-linearity, one can draw some conclusions regarding the effect of design and training choices. The main conclusion is that neural networks can be applied to non-linear image processing problems, provided that careful attention is paid to network architecture, training set sampling and parameter choice. Only if prior knowledge is used in constructing the networks and sampling the datasets can one expect to obtain a well performing neural network filter.}
}

@inproceedings{glorot2010_trainingdiff,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  editor = {Teh, Yee Whye and Titterington, Mike},
  date = {2010-05-13/2010-05-15},
  series = {Proceedings of Machine Learning Research},
  volume = {9},
  pages = {249--256},
  publisher = {PMLR},
  location = {Chia Laguna Resort, Sardinia, Italy},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@incollection{goodfellow_2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {https://www.deeplearningbook.org/contents/optimization.html}
}

@book{hastie_stat2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  publisher = {Springer},
  doi = {10.1007/978-0-387-84858-7},
  isbn = {978-0-387-84857-0 978-0-387-84858-7}
}

@book{haykin1994_ml,
  title = {Neural Networks: A Comprehensive Foundation},
  author = {Haykin, Simon},
  date = {1994},
  edition = {1},
  publisher = {Prentice Hall PTR},
  location = {USA},
  abstract = {From the Publisher:This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.},
  isbn = {0-02-352761-7}
}

@book{haykin2009_neural,
  title = {Neural Networks and Learning Machines},
  author = {Haykin, Simon},
  date = {2009},
  edition = {3rd},
  publisher = {Pearson Education},
  location = {Upper Saddle River, NJ}
}

@thesis{hochreiter1991_untersuchungen,
  type = {Diplom thesis},
  title = {Untersuchungen Zu Dynamischen Neuronalen {{Netzen}}},
  author = {Hochreiter, Sepp},
  date = {1991},
  institution = {Institut für Informatik, Technische Universität München},
  url = {https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf},
  urldate = {2024-01-01}
}

@article{hornik1991_approximation,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  date = {1991},
  journaltitle = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  publisher = {Elsevier}
}

@book{hosmer2013applied,
  title = {Applied Logistic Regression},
  author = {Hosmer, David W. and Lemeshow, Stanley and Sturdivant, Rodney X.},
  date = {2013},
  publisher = {John Wiley \& Sons}
}

@misc{kingma:adam,
  title = {{{ADAM}}: A Method for Stochastic Optimization},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {https://arxiv.org/abs/1412.6980}
}

@incollection{kohavi1998confusion,
  title = {Confusion Matrix},
  booktitle = {Encyclopedia of Machine Learning},
  author = {Kohavi, Ron and Provost, Foster},
  date = {1998},
  publisher = {Springer}
}

@article{lecun1998_tanh,
  title = {Efficient {{BackProp}}},
  author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
  date = {1998},
  journaltitle = {Neural Networks: Tricks of the Trade},
  pages = {9--48},
  publisher = {Springer}
}

@article{maas2013_leakyrelu,
  title = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
  date = {2013},
  journaltitle = {Proceedings of the 30th International Conference on Machine Learning},
  volume = {28}
}

@inproceedings{mahima2023_activfuncs,
  title = {A Comparative Analysis of the Most Commonly Used Activation Functions in Deep Neural Network},
  booktitle = {2023 4th International Conference on Electronics and Sustainable Communication Systems ({{ICESC}})},
  author = {Mahima, R. and Maheswari, M. and Roshana, S. and Priyanka, E. and Mohanan, Neha and Nandhini, N.},
  date = {2023},
  pages = {1334--1339},
  doi = {10.1109/ICESC57686.2023.10193390},
  keywords = {a leaky ReLU,Activation Function comparison,Artificial neural networks,Deep learning,Deep Neural Network,ELU,Image analysis,Image resolution,Medical services,Neurons,Nonlinearity,ReLU,Sigmoid,Softmax,Tanh,Training}
}

@book{mira1995_sigmoid,
  title = {From Natural to Artificial Neural Computation},
  editor = {Mira, José and Sandoval, Francisco},
  date = {1995},
  series = {Lecture Notes in Computer Science},
  volume = {930},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  url = {https://archive.org/details/fromnaturaltoart1995inte},
  urldate = {2024-01-01}
}

@inproceedings{mira1995_sigmoid,
  title = {From Natural to Artificial Neural Computation},
  booktitle = {International Workshop on Artificial Neural Networks},
  author = {Mira, José and Sandoval, Francisco},
  date = {1995},
  series = {{{IWANN}}},
  volume = {930},
  publisher = {Springer}
}

@inproceedings{nair2010_relu,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  date = {2010},
  pages = {807--814}
}

@inproceedings{ng2004feature,
  title = {Feature Selection, {{L1}} vs. {{L2}} Regularization, and Rotational Invariance},
  booktitle = {Proceedings of the Twenty-First International Conference on {{Machine}} Learning},
  author = {Ng, Andrew Y.},
  date = {2004},
  pages = {78},
  publisher = {ACM}
}

@misc{philipp2018_explgrads,
  title = {The Exploding Gradient Problem Demystified - Definition, Prevalence, Impact, Origin, Tradeoffs, and Solutions},
  author = {Philipp, George and Song, Dawn and Carbonell, Jaime G.},
  date = {2018},
  eprint = {1712.05577},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {https://arxiv.org/abs/1712.05577}
}

@article{powers2011evaluation,
  title = {Evaluation: From Precision, Recall and {{F-measure}} to {{ROC}}, Informedness, Markedness and Correlation},
  author = {Powers, David M. W.},
  date = {2011},
  journaltitle = {Journal of Machine Learning Technologies},
  volume = {2},
  number = {1},
  pages = {37--63}
}

@article{rosenblatt1958_perceptron,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  doi = {10.1037/h0042519}
}

@article{runge_1901,
  title = {Über Empirische {{Funktionen}} Und Die {{Interpolation}} Zwischen Äquidistanten {{Ordinaten}}},
  author = {Runge, Carl},
  date = {1901},
  journaltitle = {Zeitschrift für Mathematik und Physik},
  volume = {46},
  pages = {224--243}
}

@article{terven2025_lossfuncs,
  title = {A Comprehensive Survey of Loss Functions and Metrics in Deep Learning},
  author = {Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
  date = {2025-04-11},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artificial Intelligence Review},
  volume = {58},
  number = {7},
  pages = {195},
  issn = {1573-7462},
  doi = {10.1007/s10462-025-11198-7},
  url = {https://doi.org/10.1007/s10462-025-11198-7},
  abstract = {This paper presents a comprehensive review of loss functions and performance metrics in deep learning, highlighting key developments and practical insights across diverse application areas. We begin by outlining fundamental considerations in classic tasks such as regression and classification, then extend our analysis to specialized domains like computer vision and natural language processing including retrieval-augmented generation. In each setting, we systematically examine how different loss functions and evaluation metrics can be paired to address task-specific challenges such as class imbalance, outliers, and sequence-level optimization. Key contributions of this work include: (1) a unified framework for understanding how losses and metrics align with different learning objectives, (2) an in-depth discussion of multi-loss setups that balance competing goals, and (3) new insights into specialized metrics used to evaluate modern applications like retrieval-augmented generation, where faithfulness and context relevance are pivotal. Along the way, we highlight best practices for selecting or combining losses and metrics based on empirical behaviors and domain constraints. Finally, we identify open problems and promising directions, including the automation of loss-function search and the development of robust, interpretable evaluation measures for increasingly complex deep learning tasks. Our review aims to equip researchers and practitioners with clearer guidance in designing effective training pipelines and reliable model assessments for a wide spectrum of real-world applications.}
}

@misc{tieleman2012rmsprop,
  title = {Lecture 6.5 - {{RMSProp}}: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  date = {2012},
  volume = {4},
  pages = {26--31},
  url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  howpublished = {COURSERA: Neural Networks for Machine Learning}
}

@misc{vawieringen_ridge2023,
  title = {Lecture Notes on {{Ridge}} Regression},
  author = {Van Wieringen, Wessel N.},
  date = {2023},
  eprint = {1509.09169},
  eprinttype = {arXiv},
  eprintclass = {stat.ME},
  url = {https://arxiv.org/abs/1509.09169}
}

@incollection{wang2003_nn,
  title = {Artificial Neural Network},
  booktitle = {Interdisciplinary Computing in Java Programming},
  author = {Wang, Sun-Chong},
  date = {2003},
  pages = {81--100},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4615-0377-4_5},
  url = {https://doi.org/10.1007/978-1-4615-0377-4_5},
  abstract = {Inspired by the sophisticated functionality of human brains where hundreds of billions of interconnected neurons process information in parallel, researchers have successfully tried demonstrating certain levels of intelligence on silicon. Examples include language translation and pattern recognition software. While simulation of human consciousness and emotion is still in the realm of science fiction, we, in this chapter, consider artificial neural networks as universal function approximators. Especially, we introduce neural networks which are suited for time series forecasts.},
  isbn = {978-1-4615-0377-4}
}

@article{waterworth2000_nn,
  title = {Artificial {{Neural Networks}} in the {{Modelling}} and {{Control}} of {{Non-Linear Systems}}},
  author = {Waterworth, G. and Lees, M.},
  date = {2000-02-01},
  journaltitle = {IFAC Workshop on Programmable Devices and Systems (PDS 2000), Ostrava, Czech Republic, 8-9 February 2000},
  shortjournal = {IFAC Proceedings Volumes},
  volume = {33},
  number = {1},
  pages = {95--97},
  issn = {1474-6670},
  doi = {10.1016/S1474-6670(17)35594-5},
  url = {https://www.sciencedirect.com/science/article/pii/S1474667017355945},
  abstract = {This paper considers the application of Artificial Neural Networks (ANNs) to the control of non-linear systems, in particular considering the control of the elevation of the gun of a main battle tanle It commences with an analysis and design of a controller for a non-linear laboratory equipment, with the modelling and the design ofan ANN controller. It then reviews the modelling of a tank gun elevation and the corresponding design of a controller. A number offeatures concerning the time variance ofthe characteristics of such systems are considered including variation of parameters such as backlash, hysteresis, and saturation limits due to ageing and wear.},
  keywords = {modelling,Neural networks,non-linear control,target control}
}

@incollection{wooditch2021_ols,
  title = {Ordinary Least Squares Regression},
  booktitle = {A Beginner's Guide to Statistics for Criminology and Criminal Justice Using {{R}}},
  author = {Wooditch, Alese and Johnson, Nicole J. and Solymosi, Reka and Medina Ariza, Juanjo and Langton, Samuel},
  date = {2021},
  pages = {245--268},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-50625-4_15},
  url = {https://doi.org/10.1007/978-3-030-50625-4_15},
  abstract = {This chapter provides an introduction to ordinary least squares (OLS) regression analysis in R. This is a technique used to explore whether one or multiple variables (the independent variable or X) can predict or explain the variation in another variable (the dependent variable or Y). OLS regression belongs to a family of techniques called generalized linear models, so the variables being examined must be measured at the ratio or interval level and have a linear relationship. The chapter also reviews how to assess model fit using regression error (the difference between the predicted and actual values of Y) and R2. While you learn these techniques in R, you will be using the Crime Survey for England and Wales data from 2013 to 2014; these data derive from a face-to-face survey that asks people about their experiences of crime during the 12 months prior to interview.},
  isbn = {978-3-030-50625-4}
}
