@book{bishop2006_pattern,
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  author = {Bishop, Christopher M.},
  date = {2006},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  isbn = {0-387-31073-8}
}

@online{CompPhysicsML,
  title = {Machine Learning and Data Analysis for the Physical Sciences},
  author = {Hjorth-Jensen, Morten},
  date = {2023},
  url = {https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week35.html},
  urldate = {2024-01-20}
}

@article{cybenko1989_approx,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12-01},
  journaltitle = {Mathematics of Control, Signals and Systems},
  shortjournal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  url = {https://doi.org/10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}
}

@incollection{goodfellow_2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {MIT Press},
  url = {https://www.deeplearningbook.org/contents/optimization.html}
}

@book{hastie_stat2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  publisher = {Springer},
  doi = {10.1007/978-0-387-84858-7},
  isbn = {978-0-387-84857-0 978-0-387-84858-7}
}

@book{haykin1994_ml,
  title = {Neural Networks: A Comprehensive Foundation},
  author = {Haykin, Simon},
  date = {1994},
  edition = {1},
  publisher = {Prentice Hall PTR},
  location = {USA},
  abstract = {From the Publisher:This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.},
  isbn = {0-02-352761-7}
}

@article{hornik1991_approximation,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  date = {1991},
  journaltitle = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  publisher = {Elsevier}
}

@misc{kingma:adam,
  title = {{{ADAM}}: A Method for Stochastic Optimization},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  url = {https://arxiv.org/abs/1412.6980}
}

@inproceedings{mahima2023_activfuncs,
  title = {A Comparative Analysis of the Most Commonly Used Activation Functions in Deep Neural Network},
  booktitle = {2023 4th International Conference on Electronics and Sustainable Communication Systems ({{ICESC}})},
  author = {Mahima, R. and Maheswari, M. and Roshana, S. and Priyanka, E. and Mohanan, Neha and Nandhini, N.},
  date = {2023},
  pages = {1334--1339},
  doi = {10.1109/ICESC57686.2023.10193390},
  keywords = {a leaky ReLU,Activation Function comparison,Artificial neural networks,Deep learning,Deep Neural Network,ELU,Image analysis,Image resolution,Medical services,Neurons,Nonlinearity,ReLU,Sigmoid,Softmax,Tanh,Training}
}

@article{rosenblatt1958_perceptron,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  doi = {10.1037/h0042519}
}

@article{runge_1901,
  title = {Über Empirische {{Funktionen}} Und Die {{Interpolation}} Zwischen Äquidistanten {{Ordinaten}}},
  author = {Runge, Carl},
  date = {1901},
  journaltitle = {Zeitschrift für Mathematik und Physik},
  volume = {46},
  pages = {224--243}
}

@article{terven2025_lossfuncs,
  title = {A Comprehensive Survey of Loss Functions and Metrics in Deep Learning},
  author = {Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
  date = {2025-04-11},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artificial Intelligence Review},
  volume = {58},
  number = {7},
  pages = {195},
  issn = {1573-7462},
  doi = {10.1007/s10462-025-11198-7},
  url = {https://doi.org/10.1007/s10462-025-11198-7},
  abstract = {This paper presents a comprehensive review of loss functions and performance metrics in deep learning, highlighting key developments and practical insights across diverse application areas. We begin by outlining fundamental considerations in classic tasks such as regression and classification, then extend our analysis to specialized domains like computer vision and natural language processing including retrieval-augmented generation. In each setting, we systematically examine how different loss functions and evaluation metrics can be paired to address task-specific challenges such as class imbalance, outliers, and sequence-level optimization. Key contributions of this work include: (1) a unified framework for understanding how losses and metrics align with different learning objectives, (2) an in-depth discussion of multi-loss setups that balance competing goals, and (3) new insights into specialized metrics used to evaluate modern applications like retrieval-augmented generation, where faithfulness and context relevance are pivotal. Along the way, we highlight best practices for selecting or combining losses and metrics based on empirical behaviors and domain constraints. Finally, we identify open problems and promising directions, including the automation of loss-function search and the development of robust, interpretable evaluation measures for increasingly complex deep learning tasks. Our review aims to equip researchers and practitioners with clearer guidance in designing effective training pipelines and reliable model assessments for a wide spectrum of real-world applications.}
}

@misc{tieleman2012rmsprop,
  title = {Lecture 6.5 - {{RMSProp}}: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey},
  date = {2012},
  volume = {4},
  pages = {26--31},
  url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  howpublished = {COURSERA: Neural Networks for Machine Learning}
}

@misc{vawieringen_ridge2023,
  title = {Lecture Notes on {{Ridge}} Regression},
  author = {Van Wieringen, Wessel N.},
  date = {2023},
  eprint = {1509.09169},
  eprinttype = {arXiv},
  eprintclass = {stat.ME},
  url = {https://arxiv.org/abs/1509.09169}
}

@incollection{wang2003_nn,
  title = {Artificial Neural Network},
  booktitle = {Interdisciplinary Computing in Java Programming},
  author = {Wang, Sun-Chong},
  date = {2003},
  pages = {81--100},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4615-0377-4_5},
  url = {https://doi.org/10.1007/978-1-4615-0377-4_5},
  abstract = {Inspired by the sophisticated functionality of human brains where hundreds of billions of interconnected neurons process information in parallel, researchers have successfully tried demonstrating certain levels of intelligence on silicon. Examples include language translation and pattern recognition software. While simulation of human consciousness and emotion is still in the realm of science fiction, we, in this chapter, consider artificial neural networks as universal function approximators. Especially, we introduce neural networks which are suited for time series forecasts.},
  isbn = {978-1-4615-0377-4}
}
