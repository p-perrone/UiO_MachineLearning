% ------ PREAMBLE ------ 
\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english, french]{babel}
\usepackage[margin=2cm]{geometry}
\renewcommand{\baselinestretch}{1.15} 

\usepackage{sansmathfonts}
\usepackage{fontspec}
\setmainfont{Latin Modern Sans}

\usepackage{siunitx}
\sisetup{detect-all,
	separate-uncertainty = true,
	uncertainty-separator = {\,\pm\,},
	multi-part-units=single
}
\DeclareSIUnit{\year}{yr}
\usepackage{physics}
\usepackage{algpseudocode}
\usepackage{algorithm}
\AtBeginDocument{\RenewCommandCopy\qty\SI}
\usepackage{amsmath,amsfonts,amssymb}
\newcommand\inlineeqno{\stepcounter{equation}\ (\theequation)}
\usepackage{derivative}
\DeclareDifferential{\dd}{\mathrm{d}}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{subfig}
\usepackage{float}
\usepackage{caption}
\captionsetup{font={small, color={Gray2}}, labelfont={sf, bf}, textfont={sf,sansmath}, labelsep=colon}
\usepackage[dvipsnames]{xcolor}
\definecolor{Gray1}{RGB}{101, 101, 101}
\definecolor{Gray2}{RGB}{60, 60, 60}
\definecolor{Code}{RGB}{230, 235, 255}

% useful commands for typing ML stuff
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mb}[1]{\mathbold{#1}}
\newcommand{\T}{^{\intercal}}
\newcommand{\R}[1]{\mathbb{R}^{#1}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\lr}{^{(l)}}


\usepackage[most]{tcolorbox}
\tcbset{on line, 
	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	colframe=white,colback=Code,  
	highlight math style={enhanced}
}
\let\oldtexttt\texttt
\renewcommand{\texttt}[1]{\tcbox{\oldtexttt{#1}}}
\tcbuselibrary{theorems}

\newtcbtheorem[number within=section]{mytheo}{Theorem}%
{colback=blue!3,colframe=blue!35!white,fonttitle=\bfseries}{th}

\usepackage[colorlinks=true, 
linkcolor=Plum, 
citecolor=RoyalBlue, 
urlcolor=BlueViolet]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{pdfpages}
\usepackage{pythonhighlight}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{lipsum}
\pagestyle{fancy}
\fancyhf{} 
\fancyhead[L]{\leftmark}
\fancyfoot[C]{\thepage} 

\title{\bfseries Project 2: Building a Neural Network code\\
	\normalfont Applied Data Analysis and Machine Learning
	\\ UiO (FYS-STK4155)}
\author{
	\textbf{Pietro PERRONE}\\
	\href{pietrope@uio.no}{pietrope@uio.no} \\
}
\date{November 10, 2025}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
{\color{RoyalBlue}\titlerule[1.5pt]\LARGE\bfseries\sffamily\color{RoyalBlue}}
{\thesection}
{1em}
{}
\titleformat{\subsection}
{\Large\bfseries\sffamily\color{Gray2}}
{\thesubsection}
{1em}
{}
\titleformat{\subsubsection}
{\large\bfseries\itshape\color{Gray2}}
{\thesubsection.\alph{subsubsection}}
{1em}
{}
\titleformat{\paragraph}
{\normalfont\itshape\color{Gray2}}
{\theparagraph}
{0.5em}
{}

\usepackage[backend=biber, style=apa]{biblatex}
\addbibresource{UiO_MachineLearning.bib}

% ------ DOCUMENT ------ 

\begin{document}
	\selectlanguage{english}
	
	\begin{titlepage}
		
		\begin{figure}
			\centering
			\includegraphics[width=0.7\textwidth]{uio.png}
		\end{figure}
		
		\maketitle
		
		\centering
		
		
		\textbf{GitHub link to the project repository:\\
			\url{https://github.com/p-perrone/UiO_MachineLearning/tree/main/ml_project1}}
		
		\textbf{Link to DeepSeek chat:\\
			\url{https://chat.deepseek.com/share/h2ifare1m1c31ud8vf}}
		
		\begin{abstract}
			content...
		\end{abstract}
		
	\end{titlepage}
	\onecolumn
	
	\tableofcontents
	
	\twocolumn\


\section{Introduction}
An Artificial Neural Network is a computational model that emulates the functioning of human brain, in the way it can process several informations in parallel, resulting in a form of "intelligence" \parencite{wang2003_nn}. 

A typical Neural Network (NN) is represented in Figure \ref{fig:nn}. It generally consists of connected units, called, \emph{nodes} or \emph{neurons}. Every node receives a specific \emph{signal}, \emph{i.e.} a real number, from its connected node. Nodes can be regrouped in specific layers, and the signal travels from the input layer to the output layer, passing through an arbitrary number of \emph{hidden layers} \parencite{bishop2006_pattern}. Before being transmitted from one layer to the following one, the signal is modulated by an non-linear function, called \emph{activation function}, which can be specified for each layer. Formally, a NN transforms an input $\bold{X} \in \R{n\times d}$ into an output vector $\bold{y}$ through successive linear transformations, where every node is multiplied by \emph{weights}, and activation functions \parencite{goodfellow_2016}. The weights are the parameters used in a NN for approximating the target function or dataset. The multiplication of each node's value is usually followed by the addition of a real number (a \emph{bias}), in order to avoid the transmission of zero-signals throughout the network. A cost function is usually computed at the end, in order to assess the agreement between the target and output prediction of the NN.

The simplest possible neural network is given by the \emph{perceptron model}, conceived by \cite{rosenblatt1958_perceptron}. It is a type of linear classifier, that takes binary inputs, weights these by real numbers and yields a binary output.
More complicated neural networks can generally support a higher number of layers and nodes, accept a multidimensional input and yield a non-binary output. In this project, particularly, we focus on so called \emph{multilayer perceptrons} (MLPs). An other type of neural network can be the \emph{feed-forward} neural network (FFNN). In this model, the signal is transmitted always in a single direction, forward through the layers, from the input to the output. If each node in a layer is connected to all the other nodes in the following layer, this correspond to a \emph{fully connected} FFNN \parencite{haykin1994_ml}. The operation by which the weights and biases are optimized is called \emph{backpropagation}, that consists of calculating  the gradients of the cost function with respect to weights and biases starting from the last layer; a training can then be performed in order to minimize this gradients and find the optimal values of these parameters, using optimization algorithms such as gradient descent \parencite{goodfellow_2016}.

The aim of this project is to build a NN code in Python, that could perform both forward feeding and backpropagation. We will test this model initially on a simple 1-dimensional dataset, given by a cluster of samples computed with the Runge function for an input array $\boldsymbol{x} \in [-1, 1]$. Several types of activation functions and layers depths will be tested.

A more complicated fit will be then performed on a dataset of handwritten digits, the \href{https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784}. This consists of \num{70000} digits samples, each of size $28 \times 28$ pixels. The objective will be to train the coded NN to make it recognize and classify correctly these digits. An analysis of the goodness of such classification will also be performed.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{nn.pdf}
	\caption{\textbf{Neural Network} | A typical structure.}
	\label{fig:nn}
\end{figure}

\section{Theory and methods}

\subsection{The universal approximation theorem}
The cornerstone of the Neural Networks theory in machine learning is the \emph{universal approximation theorem}. The formulation by \parencite{cybenko1989_approx} states that:\\
\newline
{
\centering
\begin{mytheo}{Universal approximation}{theoexample}
		Let $\sigma$ be any continuous sigmoidal function such that:
		\[
		\lim_{z \to \infty} \sigma(z) = 1 \quad \text{and} \quad \lim_{z \to -\infty} \sigma(z) = 0
		\]
		Given a continuous function $F(\boldsymbol{x})$ on the unit cube $[0,1]^d$ and $\epsilon > 0$, there exists a one-hidden-layer neural network $f(\boldsymbol{x};\boldsymbol{\Theta})$ with parameters $\boldsymbol{\Theta} = (\boldsymbol{W}, \boldsymbol{b})$ where $\boldsymbol{W} \in \mathbb{R}^{m \times m}$ and $\boldsymbol{b} \in \mathbb{R}^{m}$, such that
		\[
		|F(\boldsymbol{x}) - f(\boldsymbol{x};\boldsymbol{\Theta})| < \epsilon \quad \forall \boldsymbol{x} \in [0,1]^n
		\]
	
\end{mytheo}}
\newline


In other words, any continuous function $y=F(\boldsymbol{x})$ supported on the unit cube in
$d$-dimensions can be approximated by a one-layer sigmoidal network to
arbitrary accuracy and errors.

A generalization of this theorem to any non-constant, bounded activation function was established by \parencite{hornik1991_approximation}. Dealing with the definition of the expectation value $\mathbb{E}[|F(\boldsymbol{x})|^2]$:
\begin{equation}
	\mathbb{E}[|F(\boldsymbol{x})|^2] =\int_{\boldsymbol{x}\in D} |F(\boldsymbol{x})|^2p(\boldsymbol{x})\dd{\boldsymbol{x}} < \infty.
\end{equation}
it was concluded that
	\begin{multline}
		\mathbb{E}[|F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})|^2] \\
		=\int_{\boldsymbol{x}\in D} |F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})|^2p(\boldsymbol{x})\,\dd{\boldsymbol{x}} < \epsilon.
	\end{multline}

\subsection{Structure of a Neural Network}
\begin{figure*}
	\centering
	\includegraphics[width=12cm]{simple_percptr.pdf}
	\caption{\textbf{Simple perceptron model} | In blue the different layers, in orange the functions.}
	\label{fig:simpleperc}
\end{figure*}

\subsubsection{The Feed-forward algorithm}

\paragraph{Simple perceptron model with 1-D input}
A simple FFNN will consist ofan  input vector, a single hidden layer and an output vector. We call:
\begin{itemize}
	\item $\bs{x}\T = (x_1, x_2, \dots x_n) \in \R{n}$ the input;
	\item $\bs{z}\T = (z_1, z_2, \dots z_m) \in \R{m}$ the weighted vector representing the hidden layer;
	\item $\bs{\tilde{y}}\T = (\tilde{y}_1, \tilde{y}_2, \dots \tilde{y}_m) \in \R{m}$ the activated, output layer.
\end{itemize}

The activation $y_i$ of each $i$-th neuron is a weighted sum of inputs, passed through an activation function $f$. The weights and biases correspond to the parameters of the model:
\begin{equation}
	\boldsymbol{\Theta}=(\boldsymbol{W},\boldsymbol{b})
\end{equation}

where, in this case, $\bs{W} \in \R{m\times n}$ and $\bs{b} \in \R{m}$.
In case of the simple perceptron model we have 
\begin{align}
	\boldsymbol{z} = \sum_{j=1}^m \sum_{i=1}^n w_{ij} x_i\\
	\boldsymbol{\tilde{y}} = f(\boldsymbol{z}) 
\end{align}
where $f$ is the activation function, $x_i$ represents the input from neuron $i$ in the input layer and $w_{ij}$ is the weight to input $i$. Since this is a simple perceptron, then $f(\boldsymbol{\boldsymbol{z}})$ directly corresponds to the output of the NN, which will be passed to the chosen cost function $\C(\bs{\Theta})$. This simple architecture is shown in Figure \ref{fig:simpleperc} \parencite{goodfellow_2016}.

\paragraph{Generalization to a multi-layer network with multiple inputs}
A typical neural network takes as input a matrix $\bs{X} \in \R{n \times d}$, 
where $n$ is the number of samples and $d$ is the number of features. 
For example, a network that approximates a function 
$\bs{t} = u(\bs{x}, \bs{y})$ from $100$ samples takes as input 
a matrix $[\bs{x}, \bs{y}] \in \R{100 \times 2}$.

The network can have an arbitrary number of layers 
$l = 1, 2, \dots, L$. 
We denote by 
\[
(\bs{a}\lr)\T = (a_1\lr, a_2\lr, \dots, a_{m_l}\lr)
\]
the vector of activations at layer $(l)$, 
where $m_l$ is the number of neurons in that layer.
These activations serve as input to the next layer $(l+1)$.

For a given neuron $j$ in layer $l$, the pre-activation value is
\begin{equation} \label{eq:ff}
	z_j\lr = \sum_{i=1}^{m_{l-1}} w_{ij}\lr a_i^{(l-1)} + b_j\lr,
\end{equation}
where $m_{l-1}$ is the number of neurons in the previous layer. 
In matrix form, this can be written compactly as
\begin{equation}
	\bs{Z}\lr = \bs{A}^{(l-1)} \bs{W}\lr + \bs{b}\lr,
\end{equation}
Here, we have
\begin{itemize}
	\item $\bs{A}^{(l-1)} \in \R{n \times m_{l-1}}$ : 
	activations (outputs) from the previous layer, 
	where $n$ is the number of samples and $m_{l-1}$ is the number of neurons in layer $(l-1)$;
	
	\item $\bs{W}\lr \in \R{m_{l-1} \times m_l}$ : 
	weight matrix connecting layer $(l-1)$ to layer $(l)$;
	
	\item $\bs{b}\lr \in \R{1 \times m_l}$ : 
	bias vector for layer $(l)$, added to the the $n$ samples during computation.
	
	\item $\bs{A}\lr \in \R{n \times m_l}$ : 
	activations of layer $(l)$, obtained after applying the activation function elementwise to $\bs{Z}\lr$:
	\[
	\bs{A}\lr = f\lr(\bs{Z}\lr)
	\]
\end{itemize}

\paragraph{Implementation}
A possible psudocode for such algorithm could be:
\begin{algorithm}
	\color{BlueViolet}
	\caption{Feedforward}\label{alg:ff}
	\begin{algorithmic}[1]
		\Require Input $\bs{X}$, weights $\{ \bs{W}\lr \}_{l=1}^L$, biases $\{ \bs{b}\lr \}_{l=1}^L$, activation functions $\{ f\lr \}_{l=1}^L$
		\Ensure Network output $\bs{A}\lr$
		
		\State $\bs{A}^{(0)} \gets \bs{X}$
		
		\For{$l = 1$ to $L$}
		\State $\bs{Z}\lr \gets \bs{W}\lr \, \bs{A}^{(l-1)} + \bs{B}\lr$ \Comment{Linear weighting}
		\State $\bs{A}\lr \gets f\lr(\bs{Z}\lr)$ \Comment{Activation}
		\EndFor
		
		\State \Return $\bs{A}\lr$
	\end{algorithmic}
\end{algorithm}


\subsubsection{The Backpropagation algorithm}
A consistent result from the NN calculations can only be achieved with properly trained parameters $\bs{W}$ and $\bs{b}$. As in every machine learning algorithm, the optimization of such parameters can be performed by minimizing the gradient of the cost function with respect to these same. \emph{Backpropagation} is the operation of calculating these gradients. For the layer $l$:
\begin{equation}
	\grad{\C({\bs{\Theta}\lr})} = \begin{bmatrix}
		\partial_{\bs{W}\lr} \C\\
		\partial_{\bs{b}\lr} \C
	\end{bmatrix}
\end{equation}
The output of this calculation is given by a number of gradients that correspond to the total number of layers in the NN:
\[
\left[\grad{\C({\bs{\Theta}^{(1)}})}, \grad{\C({\bs{\Theta}^{(2)}})}, \dots, \grad{\C({\bs{\Theta}^{(L)}})}\right]
\]
From Equation \eqref{eq:ff}, we know that every signal at the $j$-th node of layer $l$ depends on the output value of the activation function computed for the signal at layer $l-1$, \emph{i.e} $\bs{A}^{(l-1)}$. In turn, $\bs{A}^{(l-1)}$ is given by the activation of $\bs{Z}^{(l-1)}$, which is a function of $\bs{A}^{(l-2)}$, and so on. The dependence of each layer from the previous ones, in fully connected NNs, allows computing the gradients of the cost function with respect to the parameters of any layer by simply applying the chain rule, starting from the output layer and broadcasting to all the hidden layers in reversed order (\emph{backpropagating}, namely).

\paragraph{Formalism of backpropagation}
Let's consider a generic cost function, such as the \emph{Mean Squared Error}. For the last layer $L$, we have:
\begin{equation}
	\C(\bs{\Theta}^{(L)})  =  \frac{1}{n}\sum_{i=1}^n\left(y_i - \tilde{y}_i\right)^2=\frac{1}{n}\sum_{i=1}^n\left(a_i^{(L)} - y_i\right)^2
\end{equation}
We now want to derive the expression for the derivative of the cost function with respect to weights of the last layer, $\displaystyle \pdv{\C(\bs{\Theta}^{(L)})}{\bs{W}^{(L)}}$, and thus we can apply the chain rule:
\[
\pdv{\C}{w_{ij}^{(L)}} = \pdv{\C}{a_i^{(L)}} \pdv{a_i^{(L)}}{w_{ij}^{(L)}}
\]
The two derivatives respectively correspond to:
\begin{align}
	\pdv{\C}{a_j^{(L)}} &= a_j^{(L)} - y_j \\
	\pdv{a_j^{(L)}}{w_{ij}^{(L)}} 
	&= \pdv{a_j^{(L)}}{z_j^{(L)}} \, \pdv{z_j^{(L)}}{w_{ij}^{(L)}} 
	= a_j^{(L)} \bigl(1 - a_j^{(L)}\bigr) a_i^{(L-1)}.
\end{align}

The overall derivative, then, reads:
\begin{equation}
	\pdv{\C(\bs{\Theta}^{(L)})}{w_{ij}^{(L)}} 
	= \bigl(a_j^{(L)} - y_j\bigr) \, a_j^{(L)} \bigl(1 - a_j^{(L)}\bigr) \, a_i^{(L-1)}.
\end{equation}

In order to simplify the formalism, we can define an auxiliary factor $\bs{\delta}^{(L)}$. First, we have that, for the $j$-th node:
\begin{equation}
	\delta_j^{(L)} = a_j^{(L)} \bigl(1 - a_j^{(L)}\bigr) \bigl(a_j^{(L)} - y_j\bigr) 
	= f'\bigl(z_j^{(L)}\bigr) \pdv{\C}{a_j^{(L)}}
\end{equation}
and, using vector notation again, we have:
\begin{equation}\label{eq:deltaL}
	\bs{\delta}^{(L)} = f'(\bs{Z}^{(L)}) \odot \pdv{\C}{\bs{A}^{(L)}}
\end{equation}
where $\odot$ represents the element-wise (Hadamard) product.
Finally, we can write the derivative of the cost function with respect to the weights of the last layer:
\begin{equation}
	\pdv{\C(\bs{\Theta}^{(L)})}{w_{ij}^{(L)}} = \delta_j^{(L)} a_i^{(L-1)}
\end{equation}

For the biases, the expression can be similarly derived:
\begin{equation}
	\pdv{\C(\bs{\Theta}^{(L)})}{b_{j}^{(L)}} = \delta_j^{(L)}
\end{equation}
since the only difference from the derivative with respect to the weights is the term $\displaystyle \pdv{z_j^{(L)}}{b_j^{(L)}}$, which is, in this case, equal to 1. We then have the expression of the gradient of the cost with respect to the parameters of the last layer.

In order to obtain the expression of backpropagation at a generic layer, we can first generalize the auxiliary factor $\bs{\delta}$ at layer $l$. Expanding Equation \eqref{eq:deltaL}, we have:
\begin{multline*}
\bs{\delta\lr} = \left(f\lr\right)' \odot \left(\bs{W}^{(l+1)}\right)\T \cdot \left(f^{(l+1)}\right)' \odot \cdots\\
\cdots \odot \left( \bs{W}^{(L-1)} \right)\T \cdot \left(f^{(L-1)}\right)' \odot \left(\bs{W}^{(L)}\right) \cdot \left(f^{(L)}\right)' \odot \grad \C(\bs{A}^{(L)})
\end{multline*}
And, specifically,
\[
	\delta_j^{(l)} 
	= \sum_k 
	\pdv{\C}{z_k^{(l+1)}} 
	\pdv{z_k^{(l+1)}}{z_j^{(l)}} 
	= \sum_k 
	\delta_k^{(l+1)} 
	\pdv{z_k^{(l+1)}}{z_j^{(l)}}
\]
which basically corresponds to the application of the chain rule recursively from layer $(l+1)$ to layer $(l)$. $k$ refers to the index of any node in layers $(l+1)$. Recalling equation \eqref{eq:ff}, we can finally derive a general expression for $\delta\lr$:
\begin{equation}
	\delta_j\lr \;=\; \sum_k \delta_k^{(l+1)}\, w_{kj}^{(l+1)}\, f'\bigl(z_j\lr\bigr)
\end{equation}
and in vector notation:
\[
\bs{\delta}\lr = \bs{\delta}^{(l+1)} \bs{W}^{(l+1)} f'(\bs{Z}\lr)
\]
Finally, the gradient of the cost function with respect to weights and biases at layer $(l)$ can be expressed as follows:
\begin{equation}
	\grad{\C(\bs{\Theta}\lr)} = 
	\begin{bmatrix}
		\partial_{\bs{W}\lr} \C\\
		\partial_{\bs{b}\lr} \C
	\end{bmatrix}
	=
	\begin{bmatrix}
		\bs{\delta}\lr \bs{A}^{(l-1)} \\
		\bs{\delta}\lr
	\end{bmatrix}.
\end{equation}

\paragraph{Implementation}
Note that, in order to run backpropagation, the feedforward algorithm A possible pseudocode for the backpropagation algorithm could be:
\begin{algorithm}
	\color{BlueViolet}
	\caption{Backpropagation}\label{alg:bp}
	\begin{algorithmic}[1]
		\Require Feedforward activations $\{\bs{A}\lr\}_{l=0}^L$, pre-activations $\{\bs{Z}\lr\}_{l=1}^L$, weights $\{\bs{W}\lr\}_{l=1}^L$, cost $\C$
		\Ensure Gradients $\grad{\C(\bs{\Theta}\lr)}$
		
		\State Initialize $\bs{\delta}\lr = \mathbf{0}$ for all $l$
		\State $L \gets$ number of layers
		
		\State \Comment{Compute output layer error}
		\State $\bs{\delta}^{(L)} \gets \pdv{\C}{\bs{A}^{(L)}} \odot f^{(L)'}(\bs{Z}^{(L)})$
		
		\For{$l = L-1$ down to $1$}
		\State $\bs{\delta}\lr \gets (\bs{\delta}^{(l+1)} \bs{W}^{(l+1)\T}) \odot f^{(l)'}(\bs{Z}\lr)$
		\EndFor
		
		\State \Comment{Compute gradients for weights and biases}
		\For{$l = 1$ to $L$}
		\State $\pdv{\C}{\bs{W}\lr} \gets (\bs{A}^{(l-1)})\T \bs{\delta}\lr$
		\State $\pdv{\C}{\bs{b}\lr} \gets \sum_i \bs{\delta}\lr_i$
		\EndFor
		
		\State \Return $\left\{ \pdv{\C}{\bs{W}\lr}, \pdv{\C}{\bs{b}\lr} \right\}_{l=1}^L$
	\end{algorithmic}
\end{algorithm}

\subsubsection{Training the Neural Network}
Once the gradients have been computed, then the NN can be trained. The optimization can be performed by minimizing the gradients iteratively with machinery such as the gradient descent algorithm \parencite{hastie_stat2009}. This method has been more deeply explained in project 1's report. However, we will recall its basis and apply its formalism to NNs.

At each iterations of the gradient descent, the backpropagation (and, implicitly, the feedforward) algorithms must be called. The update rule is then given by, for each node $j$ at layer $(l)$:
\begin{align*}
w_{ij}\lr &\leftarrow  w_{ij}\lr- \eta \delta_j\lr a_i^{(l-1)}\\
b_j\lr &\leftarrow b_j\lr-\eta \delta_j\lr
\end{align*}
with $\eta$ the learning rate.

In the frame of this project, we will use Stochastic Gradient Descent, as it allows a much faster learning with bigger input datasets and multi-layer networks \parencite{goodfellow_2016}. We will test three different approach to the definition of learning rate:
\begin{itemize}
	\item constant learning rate throughout the iterations;
	\item learning rate update with the root mean of squared gradients: RMSProp \parencite{tieleman2012rmsprop};
	\item learning rate update with a combination of gradients mving averages and momentum: ADAM \parencite{kingma:adam}.
\end{itemize}



\subsubsection{Activation functions}


\subsection{Cost functions}
	
\end{document}