% ------ PREAMBLE ------ 
\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english, french]{babel}
\usepackage[margin=2cm]{geometry}
\renewcommand{\baselinestretch}{1.15} 

\usepackage{sansmathfonts}
\usepackage{fontspec}
\setmainfont{Latin Modern Sans}

\usepackage{siunitx}
\sisetup{detect-all,
	separate-uncertainty = true,
	uncertainty-separator = {\,\pm\,},
	multi-part-units=single
}
\DeclareSIUnit{\year}{yr}
\usepackage{physics}
\AtBeginDocument{\RenewCommandCopy\qty\SI}
\usepackage{amsmath,amsfonts,amssymb}
\newcommand\inlineeqno{\stepcounter{equation}\ (\theequation)}
\usepackage{derivative}
\DeclareDifferential{\dd}{\mathrm{d}}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{subfig}
\usepackage{float}
\usepackage{caption}
\captionsetup{font={small, color={Gray2}}, labelfont={sf, bf}, textfont={sf,sansmath}, labelsep=colon}
\usepackage[dvipsnames]{xcolor}
\definecolor{Gray1}{RGB}{101, 101, 101}
\definecolor{Gray2}{RGB}{60, 60, 60}
\definecolor{Code}{RGB}{230, 235, 255}

\usepackage[most]{tcolorbox}
\tcbset{on line, 
	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	colframe=white,colback=Code,  
	highlight math style={enhanced}
}
\let\oldtexttt\texttt
\renewcommand{\texttt}[1]{\tcbox{\oldtexttt{#1}}}
\tcbuselibrary{theorems}

\newtcbtheorem[number within=section]{mytheo}{Theorem}%
{colback=blue!3,colframe=blue!35!white,fonttitle=\bfseries}{th}

\usepackage[colorlinks=true, 
linkcolor=Plum, 
citecolor=RoyalBlue, 
urlcolor=BlueViolet]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{pdfpages}
\usepackage{pythonhighlight}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{lipsum}
\pagestyle{fancy}
\fancyhf{} 
\fancyhead[L]{\leftmark}
\fancyfoot[C]{\thepage} 

\title{\bfseries Project 2: Building a Neural Network code\\
	\normalfont Applied Data Analysis and Machine Learning
	\\ UiO (FYS-STK4155)}
\author{
	\textbf{Pietro PERRONE}\\
	\href{pietrope@uio.no}{pietrope@uio.no} \\
}
\date{November 10, 2025}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
{\color{RoyalBlue}\titlerule[1.5pt]\LARGE\bfseries\sffamily\color{RoyalBlue}}
{\thesection}
{1em}
{}
\titleformat{\subsection}
{\Large\bfseries\sffamily\color{Gray2}}
{\thesubsection}
{1em}
{}
\titleformat{\subsubsection}
{\large\bfseries\itshape\color{Gray2}}
{\thesubsection.\alph{subsubsection}}
{1em}
{}
\titleformat{\paragraph}
{\normalfont\itshape\color{Gray2}}
{\theparagraph}
{1em}
{}

\usepackage[backend=biber, style=apa]{biblatex}
\addbibresource{UiO_MachineLearning.bib}

% ------ DOCUMENT ------ 

\begin{document}
	\selectlanguage{english}
	
	\begin{titlepage}
		
		\begin{figure}
			\centering
			\includegraphics[width=0.7\textwidth]{uio.png}
		\end{figure}
		
		\maketitle
		
		\centering
		
		
		\textbf{GitHub link to the project repository:\\
			\url{https://github.com/p-perrone/UiO_MachineLearning/tree/main/ml_project1}}
		
		\textbf{Link to DeepSeek chat:\\
			\url{https://chat.deepseek.com/share/h2ifare1m1c31ud8vf}}
		
		\begin{abstract}
			content...
		\end{abstract}
		
	\end{titlepage}
	\onecolumn
	
	\tableofcontents
	
	\twocolumn\
	
\section{Introduction}
An Artificial Neural Network is a computational model that emulates the functioning of human brain, in the way it can process several informations in parallel, resulting in a form of "intelligence" \parencite{wang2003_nn}. 

A typical Neural Network (NN) is represented in Figure \ref{fig:nn}. It generally consists of connected units, called, \emph{nodes} or \emph{neurons}. Every node receives a specific \emph{signal}, \emph{i.e.} a real number, from its connected node. Nodes can be regrouped in specific layers, and the signal travels from the input layer to the output layer, passing through an arbitrary number of \emph{hidden layers} \parencite{bishop2006_pattern}. Before being transmitted from one layer to the following one, the signal is modulated by an non-linear function, called \emph{activation function}, which can be specified for each layer. Formally, a NN transforms an input vector $\bold{x} \in \mathbb{R}^d$ into an output vector $\bold{y}$ through successive linear transformations, where every node is multiplied by \emph{weights}, and activation functions \parencite{goodfellow_2016}. The weights are the parameters used in a NN for approximating the target function or dataset. The multiplication of each node's value is usually followed by the addition of a real number (a \emph{bias}), in order to avoid the transmission of zero-signals throughout the network. A cost function is usually computed at the end, in order to assess the agreement between the target and output prediction of the NN.

The simplest possible neural network is given by the \emph{perceptron model}, conceived by \cite{rosenblatt1958_perceptron}. It is a type of linear classifier, that takes binary inputs, weights these by real numbers and yields a binary output.
More complicated neural networks can generally support a higher number of layers and nodes, accept a multidimensional input and yield a non-binary output. In this project, particularly, we focus on so called \emph{multilayer perceptrons} (MLPs). An other type of neural network can be the \emph{feed-forward} neural network (FFNN). In this model, the signal is transmitted always in a single direction, forward through the layers, from the input to the output. If each node in a layer is connected to all the other nodes in the following layer, this correspond to a \emph{fully connected} FFNN \parencite{haykin1994_ml}. The operation by which the weights and biases are optimized is called \emph{backpropagation}, that consists of calculating  the gradients of the cost function with respect to weights and biases starting from the last layer; a training can then be performed in order to minimize this gradients and find the optimal values of these parameters, using optimization algorithms such as gradient descent \parencite{goodfellow_2016}.

The aim of this project is to build a NN code in Python, that could perform both forward feeding and backpropagation. We will test this model initially on a simple 1-dimensional dataset, given by a cluster of samples computed with the Runge function for an input array $\boldsymbol{x} \in [-1, 1]$. Several types of activation functions and layers depths will be tested.

A more complicated fit will be then performed on a dataset of handwritten digits, the \href{https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784}. This consists of \num{70000} digits samples, each of size $28 \times 28$ pixels. The objective will be to train the coded NN to make it recognize and classify correctly these digits. An analysis of the goodness of such classification will also be performed.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{nn.pdf}
	\caption{\textbf{Neural Network} | A typical structure.}
	\label{fig:nn}
\end{figure}

\section{Theory and methods}

\subsection{The universal approximation theorem}
The cornerstone of the Neural Networks theory in machine learning is the \emph{universal approximation theorem}. The formulation by \parencite{cybenko1989_approx} states that:\\
\newline
{
\centering
\begin{mytheo}{This is my title}{theoexample}
		Let $\sigma$ be any continuous sigmoidal function such that:
		\[
		\lim_{z \to \infty} \sigma(z) = 1 \quad \text{and} \quad \lim_{z \to -\infty} \sigma(z) = 0
		\]
		Given a continuous function $F(\boldsymbol{x})$ on the unit cube $[0,1]^d$ and $\epsilon > 0$, there exists a one-hidden-layer neural network $f(\boldsymbol{x};\boldsymbol{\Theta})$ with parameters $\boldsymbol{\Theta} = (\boldsymbol{W}, \boldsymbol{b})$ where $\boldsymbol{W} \in \mathbb{R}^{m \times d}$ and $\boldsymbol{b} \in \mathbb{R}^{m}$, such that
		\[
		|F(\boldsymbol{x}) - f(\boldsymbol{x};\boldsymbol{\Theta})| < \epsilon \quad \forall \boldsymbol{x} \in [0,1]^d
		\]
	
\end{mytheo}}
\newline

\begin{figure*}
	\centering
	\includegraphics[width=12cm]{mnist_cm}
\end{figure*}

In other words, any continuous function $y=F(\boldsymbol{x})$ supported on the unit cube in
$d$-dimensions can be approximated by a one-layer sigmoidal network to
arbitrary accuracy and errors.

A generalization of this theorem to any non-constant, bounded activation function was established by \parencite{hornik1991_approximation}. Dealing with the definition of the expectation value $\mathbb{E}[|F(\boldsymbol{x})|^2]$:
\begin{equation}
	\mathbb{E}[|F(\boldsymbol{x})|^2] =\int_{\boldsymbol{x}\in D} |F(\boldsymbol{x})|^2p(\boldsymbol{x})\,d \boldsymbol{x} < \infty.
\end{equation}
it was concluded that
	\begin{multline}
		\mathbb{E}[|F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})|^2] \\
		=\int_{\boldsymbol{x}\in D} |F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})|^2p(\boldsymbol{x})\,d\boldsymbol{x} < \epsilon.
	\end{multline}

\subsection{Structure of a Neural Network}
As stated previously, the most generalized structure of a typical neural network consists of an input layer, a given number of hidden layers and an output layer. In every layer $l$ each node is modulated by the weights and biases, which corresponds to the parameters of this model:
\begin{equation}
	\boldsymbol{\Theta}^{(l)}=(\boldsymbol{W}^{(l)},\boldsymbol{b}^{(l)})
\end{equation}
and scaled by and activation function.



 


\subsubsection{Activation functions}

\subsubsection{The Feedforward algorithm}

\subsubsection{The Backpropagation algorithm}

\subsubsection{Training the Neural Network}

\subsection{Cost functions}
	
\end{document}