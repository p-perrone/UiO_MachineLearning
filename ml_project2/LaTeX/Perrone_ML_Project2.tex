% ------ PREAMBLE ------ 
\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[english, french]{babel}
\usepackage[margin=2cm]{geometry}
\renewcommand{\baselinestretch}{1.05} 

\usepackage{sansmathfonts}
\usepackage{fontspec}
\setmainfont{Latin Modern Sans}

\usepackage{siunitx}
\sisetup{detect-all,
	separate-uncertainty = true,
	uncertainty-separator = {\,\pm\,},
	multi-part-units=single
}
\DeclareSIUnit{\year}{yr}
\usepackage{physics}
\usepackage{algpseudocode}
\usepackage{algorithm}
\AtBeginDocument{\RenewCommandCopy\qty\SI}
\usepackage{amsmath,amsfonts,amssymb}
\newcommand\inlineeqno{\stepcounter{equation}\ (\theequation)}
\usepackage{derivative}
\DeclareDifferential{\dd}{\mathrm{d}}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{subfig}
\usepackage{float}
\usepackage{caption}
\captionsetup{font={small, color={Gray2}}, labelfont={sf, bf}, textfont={sf,sansmath}, labelsep=colon}
\usepackage[dvipsnames]{xcolor}
\definecolor{Gray1}{RGB}{101, 101, 101}
\definecolor{Gray2}{RGB}{60, 60, 60}
\definecolor{Code}{RGB}{230, 235, 255}

% useful commands for typing ML 
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mb}[1]{\mathbold{#1}}
\newcommand{\T}{^{\intercal}}
\newcommand{\R}[1]{\mathbb{R}^{#1}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\lr}{^{(l)}}
\newcommand{\sk}{\texttt{scikit-learn}}
\newcommand{\py}[1]{\texttt{#1}}
\newcommand{\ie}{\emph{i.e.} }


\usepackage[most]{tcolorbox}
\tcbset{on line, 
	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	colframe=white,colback=Code,  
	highlight math style={enhanced}
}
\let\oldtexttt\texttt
\renewcommand{\texttt}[1]{\tcbox[boxsep=1pt, left=2pt, right=2pt]{\oldtexttt{#1}}}

\tcbuselibrary{theorems}
\newtcbtheorem[number within=section]{mytheo}{Theorem}%
{colback=blue!3,colframe=blue!35!white,fonttitle=\bfseries}{th}

\usepackage[colorlinks=true, 
linkcolor=Plum, 
citecolor=RoyalBlue, 
urlcolor=BlueViolet]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{pdfpages}
\usepackage{pythonhighlight}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{lipsum}
\pagestyle{fancy}
\fancyhf{} 
\fancyhead[L]{\leftmark}
\fancyfoot[C]{\thepage} 

\title{\bfseries Project 2: Building a Neural Network code\\
	\normalfont Applied Data Analysis and Machine Learning
	\\ UiO (FYS-STK4155)}
\author{
	\textbf{Pietro PERRONE}\\
	\href{pietrope@uio.no}{pietrope@uio.no} \\
}
\date{November 10, 2025}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}
{\color{RoyalBlue}\titlerule[1.5pt]\LARGE\fontseries{sbc}\sffamily\selectfont\color{RoyalBlue}}
{\thesection}
{1em}
{}
\titleformat{\subsection}
{\Large\fontseries{sbc}\selectfont\sffamily\color{Gray2}}
{\thesubsection}
{1em}
{}
\titleformat{\subsubsection}
{\large\fontseries{sbc}\slshape\sffamily\color{Gray2}}
{\thesubsection.\alph{subsubsection}}
{1em}
{}
\titleformat{\paragraph}
{\normalfont\fontseries{sbc}\slshape\sffamily\color{Gray2}}
{\theparagraph}
{0.5em}
{}

\usepackage[backend=biber, style=apa]{biblatex}
\addbibresource{UiO_MachineLearning.bib}

% ------ DOCUMENT ------ 

\begin{document}
	\selectlanguage{english}
	
	\begin{titlepage}
		
		\begin{figure}
			\centering
			\includegraphics[width=0.7\textwidth]{uio.png}
		\end{figure}
		
		\maketitle
		
		\centering
		
		
		\textbf{GitHub link to the project repository:\\
			\url{https://github.com/p-perrone/UiO_MachineLearning/tree/main/ml_project2}}
		
		\textbf{Link to DeepSeek chat:\\
			\url{https://chat.deepseek.com/share/h2ifare1m1c31ud8vf}}
		
		\vspace{2cm}
		
		\begin{abstract}
				AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\\
				AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\\AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
		\end{abstract}
		
	\end{titlepage}
	\onecolumn
	
	\tableofcontents
	
	\twocolumn

\section{Introduction}
\begin{figure}
	\centering
	\includegraphics[width=0.9\columnwidth]{nn.pdf}
	\caption{\textbf{Neural Network} | A typical structure.}
	\label{fig:nn}
\end{figure}
An Artificial Neural Network is a computational model that emulates the functioning of the human brain, in the way it can process several pieces of information in parallel, resulting in a form of "intelligence" \parencite{wang2003_nn}. 

A typical Neural Network (NN) is represented in Figure \ref{fig:nn}. It generally consists of connected units, called \emph{nodes} or \emph{neurons}. Every node receives a specific \emph{signal}, \emph{i.e.}, a real number, from its connected nodes. Nodes can be grouped into specific layers, and the signal travels from the input layer to the output layer, passing through an arbitrary number of \emph{hidden layers} \parencite{bishop2006_pattern}. Before being transmitted from one layer to the following one, the signal is modulated by a non-linear function, called the \emph{activation function}, which can be specified for each layer. Formally, an NN transforms an input $\bold{X} \in \R{n\times d}$ into an output vector $\bold{y}$ through successive linear transformations, where every node is multiplied by \emph{weights}, and activation functions are applied \parencite{goodfellow_2016}. The weights are the parameters used in an NN for approximating the target function or dataset. The multiplication of each node's value is usually followed by the addition of a real number (a \emph{bias}), in order to avoid the transmission of zero signals throughout the network. A cost function is usually computed at the end, in order to assess the agreement between the target and output prediction of the NN.

The simplest possible neural network is given by the \emph{perceptron model}, conceived by \textcite{rosenblatt1958_perceptron}. It is a type of linear classifier that takes binary inputs, weights them by real numbers, and yields a binary output.
More complicated neural networks can generally support a higher number of layers and nodes, accept a multidimensional input, and yield a non-binary output. In this project, in particular, we focus on the so-called \emph{multilayer perceptrons} (MLPs). Another type of neural network can be the \emph{feed-forward} neural network (FFNN). In this model, the signal is transmitted always in a single direction, forward through the layers, from the input to the output. If each node in a layer is connected to all the other nodes in the following layer, this corresponds to a \emph{fully connected} FFNN \parencite{haykin1994_ml}. The operation by which the weights and biases are optimized is called \emph{backpropagation}, which consists of calculating the gradients of the cost function with respect to weights and biases starting from the last layer; training can then be performed in order to minimize these gradients and find the optimal values of these parameters, using optimization algorithms such as gradient descent \parencite{goodfellow_2016}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{runge}
	\caption{\textbf{Runge function} | In orange with an added noise of normal distribution $\mathcal{N}$(0, 0.1). The $x$ values have already been scaled.}
	\label{fig:runge}
\end{figure} 

The aim of this project is to build NN code in Python that can perform both forward feeding and backpropagation. We will test this model initially on a simple one-dimensional dataset, given by a cluster of samples computed with the Runge function for an input array $\boldsymbol{x} \in [-1, 1]$ (Figure \ref{fig:runge}). Several types of activation functions and layer depths will be tested.

A more complicated fit will then be performed on a dataset of handwritten digits, the \href{https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784}. This consists of \num{70000} digit samples, each of size $28 \times 28$ pixels. The objective will be to train the coded NN to make it recognize and classify these digits correctly. An analysis of the goodness of such classification will also be performed.


\section{Theory and methods}

\subsection{The universal approximation theorem}
The cornerstone of Neural Network theory in machine learning is the \emph{universal approximation theorem}. The formulation by \parencite{cybenko1989_approx} states that:\\
\newline
{
	\centering
	\begin{mytheo}{Universal approximation}{theoexample}
		Let $\sigma$ be any continuous sigmoidal function such that:
		\[
		\lim_{z \to \infty} \sigma(z) = 1 \quad \text{and} \quad \lim_{z \to -\infty} \sigma(z) = 0
		\]
		Given a continuous function $F(\boldsymbol{x})$ on the unit cube $[0,1]^d$ and $\epsilon > 0$, there exists a one-hidden-layer neural network $f(\boldsymbol{x};\boldsymbol{\Theta})$ with parameters $\boldsymbol{\Theta} = (\boldsymbol{W}, \boldsymbol{b})$ where $\boldsymbol{W} \in \mathbb{R}^{m \times m}$ and $\boldsymbol{b} \in \mathbb{R}^{m}$, such that
		\[
		|F(\boldsymbol{x}) - f(\boldsymbol{x};\boldsymbol{\Theta})| < \epsilon \quad \forall \boldsymbol{x} \in [0,1]^n
		\]
		
\end{mytheo}}
\newline


In other words, any continuous function $y=F(\boldsymbol{x})$ supported on the unit cube in
$d$ dimensions can be approximated by a one-layer sigmoidal network to
arbitrary accuracy and errors.

A generalization of this theorem to any non-constant, bounded activation function was established by \parencite{hornik1991_approximation}. Dealing with the definition of the expectation value $\mathbb{E}[|F(\boldsymbol{x})|^2]$:
\begin{equation}
	\mathbb{E}[|F(\boldsymbol{x})|^2] =\int_{\boldsymbol{x}\in D} |F(\boldsymbol{x})|^2p(\boldsymbol{x})\dd{\boldsymbol{x}} < \infty.
\end{equation}
it was concluded that
\begin{multline}
	\mathbb{E}[|F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})|^2] \\
	=\int_{\boldsymbol{x}\in D} |F(\boldsymbol{x})-f(\boldsymbol{x};\boldsymbol{\Theta})|^2p(\boldsymbol{x})\,\dd{\boldsymbol{x}} < \epsilon.
\end{multline}

\subsection{Structure of a Neural Network}
\begin{figure*}
	\centering
	\includegraphics[width=12cm]{simple_percptr.pdf}
	\caption{\textbf{Simple perceptron model} | In blue the different layers, in orange the functions.}
	\label{fig:simpleperc}
\end{figure*}

\subsubsection{The Feed-forward algorithm}

\paragraph{Simple perceptron model with 1-D input}
A simple FFNN will consist of an input vector, a single hidden layer, and an output vector. We call:
\begin{itemize}
	\item $\bs{x}\T = (x_1, x_2, \dots x_n) \in \R{n}$ the input;
	\item $\bs{z}\T = (z_1, z_2, \dots z_m) \in \R{m}$ the weighted vector representing the hidden layer;
	\item $\bs{\tilde{y}}\T = (\tilde{y}_1, \tilde{y}_2, \dots \tilde{y}_m) \in \R{m}$ the activated, output layer.
\end{itemize}

The activation $y_i$ of each $i$-th neuron is a weighted sum of inputs, passed through an activation function $f$. The weights and biases correspond to the parameters of the model:
\begin{equation}
	\boldsymbol{\Theta}=(\boldsymbol{W},\boldsymbol{b})
\end{equation}

where, in this case, $\bs{W} \in \R{m\times n}$ and $\bs{b} \in \R{m}$.
In the case of the simple perceptron model we have 
\begin{align}
	\boldsymbol{z} = \sum_{j=1}^m \sum_{i=1}^n w_{ij} x_i\\
	\boldsymbol{\tilde{y}} = f(\boldsymbol{z}) 
\end{align}
where $f$ is the activation function, $x_i$ represents the input from neuron $i$ in the input layer, and $w_{ij}$ is the weight to input $i$. Since this is a simple perceptron, then $f(\boldsymbol{z})$ directly corresponds to the output of the NN, which will be passed to the chosen cost function $\C(\bs{\Theta})$. This simple architecture is shown in Figure \ref{fig:simpleperc} \parencite{goodfellow_2016}.

\paragraph{Generalization to a multi-layer network with multiple inputs}
A typical neural network takes as input a matrix $\bs{X} \in \R{n \times d}$, 
where $n$ is the number of samples and $d$ is the number of features. 
For example, a network that approximates a function 
$\bs{t} = u(\bs{x}, \bs{y})$ from $100$ samples takes as input 
a matrix $[\bs{x}, \bs{y}] \in \R{100 \times 2}$.

The network can have an arbitrary number of layers 
$l = 1, 2, \dots, L$. 
We denote by 
\[
(\bs{a}\lr)\T = (a_1\lr, a_2\lr, \dots, a_{m_l}\lr)
\]
the vector of activations at layer $l$, 
where $m_l$ is the number of neurons in that layer.
These activations serve as input to the next layer $l+1$.

For a given neuron $j$ in layer $l$, the pre-activation value is
\begin{equation} \label{eq:ff}
	z_j\lr = \sum_{i=1}^{m_{l-1}} w_{ij}\lr a_i^{(l-1)} + b_j\lr,
\end{equation}
where $m_{l-1}$ is the number of neurons in the previous layer. 
In matrix form, this can be written compactly as
\begin{equation}
	\bs{Z}\lr = \bs{A}^{(l-1)} \bs{W}\lr + \bs{b}\lr,
\end{equation}
Here, we have
\begin{itemize}
	\item $\bs{A}^{(l-1)} \in \R{n \times m_{l-1}}$ : 
	activations (outputs) from the previous layer, 
	where $n$ is the number of samples and $m_{l-1}$ is the number of neurons in layer $l-1$;
	
	\item $\bs{W}\lr \in \R{m_{l-1} \times m_l}$ : 
	weight matrix connecting layer $l-1$ to layer $l$;
	
	\item $\bs{b}\lr \in \R{1 \times m_l}$ : 
	bias vector for layer $l$, added to the $n$ samples during computation;
	
	\item $\bs{A}\lr \in \R{n \times m_l}$ : 
	activations of layer $l$, obtained after applying the activation function elementwise to $\bs{Z}\lr$:
	\[
	\bs{A}\lr = f\lr(\bs{Z}\lr)
	\]
\end{itemize}

\paragraph{Implementation}
A possible pseudocode for such an algorithm could be:
\begin{algorithm}
	\color{BlueViolet}
	\caption{Feedforward}\label{alg:ff}
	\begin{algorithmic}[1]
		\Require Input $\bs{X}$, weights $\{ \bs{W}\lr \}_{l=1}^L$, biases $\{ \bs{b}\lr \}_{l=1}^L$, activation functions $\{ f\lr \}_{l=1}^L$
		\Ensure Network output $\bs{A}\lr$
		
		\State $\bs{A}^{(0)} \gets \bs{X}$
		
		\For{$l = 1$ to $L$}
		\State $\bs{Z}\lr \gets \bs{W}\lr \, \bs{A}^{(l-1)} + \bs{B}\lr$ \Comment{Linear weighting}
		\State $\bs{A}\lr \gets f\lr(\bs{Z}\lr)$ \Comment{Activation}
		\EndFor
		
		\State \Return $\bs{A}\lr$
	\end{algorithmic}
\end{algorithm}


\subsubsection{The Backpropagation algorithm}
A consistent result from the NN calculations can only be achieved with properly trained parameters $\bs{W}$ and $\bs{b}$. As in every machine learning algorithm, the optimization of such parameters can be performed by minimizing the gradient of the cost function with respect to these parameters. \emph{Backpropagation} is the operation of calculating these gradients. For the layer $l$:
\begin{equation}
	\grad{\C({\bs{\Theta}\lr})} = \begin{bmatrix}
		\partial_{\bs{W}\lr} \C\\
		\partial_{\bs{b}\lr} \C
	\end{bmatrix}
\end{equation}
The output of this calculation is given by a number of gradients that correspond to the total number of layers in the NN:
\[
\left[\grad{\C({\bs{\Theta}^{(1)}})}, \grad{\C({\bs{\Theta}^{(2)}})}, \dots, \grad{\C({\bs{\Theta}^{(L)}})}\right]
\]
From Equation \eqref{eq:ff}, we know that every signal at the $j$-th node of layer $l$ depends on the output value of the activation function computed for the signal at layer $l-1$, \emph{i.e.} $\bs{A}^{(l-1)}$. In turn, $\bs{A}^{(l-1)}$ is given by the activation of $\bs{Z}^{(l-1)}$, which is a function of $\bs{A}^{(l-2)}$, and so on. The dependence of each layer on the previous ones, in fully connected NNs, allows computing the gradients of the cost function with respect to the parameters of any layer by simply applying the chain rule, starting from the output layer and propagating to all the hidden layers in reversed order (\emph{backpropagating}, namely).

\paragraph{Formalism of backpropagation}
Let's consider a generic cost function, such as the \emph{Mean Squared Error}. For the last layer $L$, we have:
\begin{equation}
	\C(\bs{\Theta}^{(L)})  =  \frac{1}{n}\sum_{i=1}^n\left(y_i - \tilde{y}_i\right)^2=\frac{1}{n}\sum_{i=1}^n\left(a_i^{(L)} - y_i\right)^2
\end{equation}
We now want to derive the expression for the derivative of the cost function with respect to weights of the last layer, $\displaystyle \pdv{\C(\bs{\Theta}^{(L)})}{\bs{W}^{(L)}}$, and thus we can apply the chain rule:
\[
\pdv{\C}{w_{ij}^{(L)}} = \pdv{\C}{a_i^{(L)}} \pdv{a_i^{(L)}}{w_{ij}^{(L)}}
\]
The two derivatives respectively correspond to:
\begin{align}
	\pdv{\C}{a_j^{(L)}} &= a_j^{(L)} - y_j \\
	\pdv{a_j^{(L)}}{w_{ij}^{(L)}} 
	&= \pdv{a_j^{(L)}}{z_j^{(L)}} \, \pdv{z_j^{(L)}}{w_{ij}^{(L)}} 
	= a_j^{(L)} \bigl(1 - a_j^{(L)}\bigr) a_i^{(L-1)}.
\end{align}

The overall derivative, then, reads:
\begin{equation}
	\pdv{\C(\bs{\Theta}^{(L)})}{w_{ij}^{(L)}} 
	= \bigl(a_j^{(L)} - y_j\bigr) \, a_j^{(L)} \bigl(1 - a_j^{(L)}\bigr) \, a_i^{(L-1)}.
\end{equation}

In order to simplify the formalism, we can define an auxiliary factor $\bs{\delta}^{(L)}$. First, we have that, for the $j$-th node:
\begin{equation}
	\delta_j^{(L)} = a_j^{(L)} \bigl(1 - a_j^{(L)}\bigr) \bigl(a_j^{(L)} - y_j\bigr) 
	= f'\bigl(z_j^{(L)}\bigr) \pdv{\C}{a_j^{(L)}}
\end{equation}
and, using vector notation again, we have:
\begin{equation}\label{eq:deltaL}
	\bs{\delta}^{(L)} = f'(\bs{Z}^{(L)}) \odot \pdv{\C}{\bs{A}^{(L)}}
\end{equation}
where $\odot$ represents the element-wise (Hadamard) product.
Finally, we can write the derivative of the cost function with respect to the weights of the last layer:
\begin{equation}
	\pdv{\C(\bs{\Theta}^{(L)})}{w_{ij}^{(L)}} = \delta_j^{(L)} a_i^{(L-1)}
\end{equation}

For the biases, the expression can be similarly derived:
\begin{equation}
	\pdv{\C(\bs{\Theta}^{(L)})}{b_{j}^{(L)}} = \delta_j^{(L)}
\end{equation}
since the only difference from the derivative with respect to the weights is the term $\displaystyle \pdv{z_j^{(L)}}{b_j^{(L)}}$, which is, in this case, equal to 1. We then have the expression of the gradient of the cost with respect to the parameters of the last layer.

In order to obtain the expression of backpropagation at a generic layer, we can first generalize the auxiliary factor $\bs{\delta}$ at layer $l$. Expanding Equation \eqref{eq:deltaL}, we have:
\begin{multline*}
	\bs{\delta\lr} = \left(f\lr\right)' \odot \left(\bs{W}^{(l+1)}\right)\T \cdot \left(f^{(l+1)}\right)' \odot \cdots\\
	\cdots \odot \left( \bs{W}^{(L-1)} \right)\T \cdot \left(f^{(L-1)}\right)' \odot \left(\bs{W}^{(L)}\right) \cdot \left(f^{(L)}\right)' \odot \grad \C(\bs{A}^{(L)})
\end{multline*}
And, specifically,
\[
\delta_j^{(l)} 
= \sum_k 
\pdv{\C}{z_k^{(l+1)}} 
\pdv{z_k^{(l+1)}}{z_j^{(l)}} 
= \sum_k 
\delta_k^{(l+1)} 
\pdv{z_k^{(l+1)}}{z_j^{(l)}}
\]
which basically corresponds to the application of the chain rule recursively from layer $l+1$ to layer $l$. $k$ refers to the index of any node in layer $l+1$. Recalling Equation \eqref{eq:ff}, we can finally derive a general expression for $\delta\lr$:
\begin{equation}
	\delta_j\lr \;=\; \sum_k \delta_k^{(l+1)}\, w_{kj}^{(l+1)}\, f'\bigl(z_j\lr\bigr)
\end{equation}
and in vector notation:
\[
\bs{\delta}\lr = \bs{\delta}^{(l+1)} \bs{W}^{(l+1)} f'(\bs{Z}\lr)
\]
Finally, the gradient of the cost function with respect to weights and biases at layer $l$ can be expressed as follows:
\begin{equation}
	\grad{\C(\bs{\Theta}\lr)} = 
	\begin{bmatrix}
		\partial_{\bs{W}\lr} \C\\
		\partial_{\bs{b}\lr} \C
	\end{bmatrix}
	=
	\begin{bmatrix}
		\bs{\delta}\lr \bs{A}^{(l-1)} \\
		\bs{\delta}\lr
	\end{bmatrix}.
\end{equation}

\paragraph{Implementation}
Note that, in order to run backpropagation, the feedforward algorithm A possible pseudocode for the backpropagation algorithm could be:
\begin{algorithm}
	\color{BlueViolet}
	\caption{Backpropagation}\label{alg:bp}
	\begin{algorithmic}[1]
		\Require Feedforward activations $\{\bs{A}\lr\}_{l=0}^L$, pre-activations $\{\bs{Z}\lr\}_{l=1}^L$, weights $\{\bs{W}\lr\}_{l=1}^L$, cost $\C$
		\Ensure Gradients $\grad{\C(\bs{\Theta}\lr)}$
		
		\State Initialize $\bs{\delta}\lr = \mathbf{0}$ for all $l$
		\State $L \gets$ number of layers
		
		\State \Comment{Compute output layer error}
		\State $\bs{\delta}^{(L)} \gets \pdv{\C}{\bs{A}^{(L)}} \odot f^{(L)'}(\bs{Z}^{(L)})$
		
		\For{$l = L-1$ down to $1$}
		\State $\bs{\delta}\lr \gets (\bs{\delta}^{(l+1)} \bs{W}^{(l+1)\T}) \odot f^{(l)'}(\bs{Z}\lr)$
		\EndFor
		
		\State \Comment{Compute gradients for weights and biases}
		\For{$l = 1$ to $L$}
		\State $\pdv{\C}{\bs{W}\lr} \gets (\bs{A}^{(l-1)})\T \bs{\delta}\lr$
		\State $\pdv{\C}{\bs{b}\lr} \gets \sum_i \bs{\delta}\lr_i$
		\EndFor
		
		\State \Return $\left\{ \pdv{\C}{\bs{W}\lr}, \pdv{\C}{\bs{b}\lr} \right\}_{l=1}^L$
	\end{algorithmic}
\end{algorithm}

\subsubsection{Training the Neural Network}
Once the gradients have been computed, then the NN can be trained. The optimization can be performed by minimizing the gradients iteratively with machinery such as the gradient descent algorithm \parencite{hastie_stat2009}. This method has been more deeply explained in project 1's report. However, we will recall its basis and apply its formalism to NNs.

At each iterations of the gradient descent, the backpropagation (and, implicitly, the feedforward) algorithms must be called. The update rule is then given by, for each node $j$ at layer $(l)$:
\begin{align*}
w_{ij}\lr &\leftarrow  w_{ij}\lr- \eta \delta_j\lr a_i^{(l-1)}\\
b_j\lr &\leftarrow b_j\lr-\eta \delta_j\lr
\end{align*}
with $\eta$ the learning rate.

In the frame of this project, we will use Stochastic Gradient Descent, as it allows a much faster learning with bigger input datasets and multi-layer networks \parencite{goodfellow_2016}. This method consists in performing the learning on a different subset of the whole dataset (\emph{batch}) at each iteration, randomly defined and of constant size $m$. The iterative training scheme is summarized in the following pseudocode. Training is performed over all batches, which are defined by a shuffled index, and repeated for a number of iterations referred to as \emph{epochs}.
\begin{algorithm}
	\color{BlueViolet}
	\caption{Stochastic Gradient Descent training}
	\begin{algorithmic}[1]
		\Require training data $\bs{X}, \bs{y}$, cost $\C$, learning rate $\eta$
		\Ensure optimized parameters
		\For{each epoch}
		\State shuffle indices for batches
		\For{each batch $(\bold{x}_b, \bold{y}_b)$}
		\State backpropagate $\bs{g} \leftarrow \grad{\C(\bs{\Theta})}$
		\State update parameters $\bs{\Theta} \leftarrow \bs{\Theta} - \eta \bs{g}$
		\EndFor
		\EndFor
		\State \Return $\bs{\Theta}$
	\end{algorithmic}
\end{algorithm}

We will test three different approach to the definition of the learning rate:
\begin{itemize}
	\item constant learning rate throughout the iterations;
	\item learning rate update with the root means of squared gradients: RMSProp \parencite{tieleman2012rmsprop};
	\item learning rate update with a combination of gradients moving averages and momentum: ADAM \parencite{kingma:adam}.
\end{itemize}

\subsection{Activation functions}
So far, we addressed the activation function of the specific layer $(l)$ with a general expression $f\lr$. Depending on the objective of the model and the types of used data, different activation function can actually be employed. We will present five of these, that will be used in our code. In Figure \ref{fig:af} all these activation functions have been plotted, to compare their trends.
\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{activation_functions.pdf}
	\caption{\textbf{Activation functions} | A representation of each function}
	\label{fig:af}
\end{figure*}

\subsubsection{Activation functions and numerical instabilities}
Some of this functions have been found to cause numerical instabilities. During backpropagation, the gradients can get smaller and smaller as the algorithm progresses down to the first hidden layers. As a result, the gradient descent update leaves the gradients of the lower hidden layers basically unchanged, causing the algorithm to never converge to a consistent solution. This is known in the literature as \emph{vanishing gradients problem} \parencite{hochreiter1991_untersuchungen}. The opposite problem can also occur if the gradients exponentially grow from the last layer to the lower ones; in this case, we talk about \emph{exploding gradients} \parencite{philipp2018_explgrads}. Recently, a study from \cite{glorot2010_trainingdiff} stated that most of this numerical instabilities were caused by a improper random initialization of the weights at the beginning of backpropagation.

\subsubsection{Employed activation functions}
\paragraph{Rectified Linear Unit (ReLU)}
The ReLU activation is defined as:
\begin{equation}
	\text{ReLU}(z) = \max(0, z)
\end{equation}
and its derivative reads
\begin{equation}
	\text{ReLU}'(z) = \begin{cases} 
		1 & \text{if } z > 0 \\
		0 & \text{if } z \leq 0 
	\end{cases}
\end{equation}
ReLU avoids vanishing gradients for positive inputs and is computationally efficient \parencite{nair2010_relu}. However, it can suffer from "dying ReLU" problems where neurons become inactive \parencite{maas2013_leakyrelu}.

\paragraph{Leaky ReLU}
The Leaky ReLU is defined as:
\begin{equation}
	\text{LeakyReLU}(z) = \begin{cases} 
		z & \text{if } z > 0 \\
		\alpha z & \text{if } z \leq 0 
	\end{cases}
\end{equation}
and its derivative reads
\begin{equation}
	\text{LeakyReLU}'(z) = \begin{cases} 
		1 & \text{if } z > 0 \\
		\alpha & \text{if } z \leq 0 
	\end{cases}
\end{equation}
where $\alpha$ is a small positive constant (typically 0.01). This activation prevents dead neurons by allowing a small gradient when $z \leq 0$, thanks to the modulation of parameter $\alpha$ \parencite{maas2013_leakyrelu}.

\paragraph{Exponential Linear Unit (ELU)}
The ELU activation is defined as:
\begin{equation}
	\text{ELU}(z) = \begin{cases} 
		z & \text{if } z >  \\
		\alpha(e^z - 1) & \text{if } z \leq 0 
	\end{cases}
\end{equation}
and its derivative reads
\begin{equation}
	\text{ELU}'(z) = \begin{cases} 
		1 & \text{if } z > 0 \\
		\text{ELU}(z) + \alpha & \text{if } z \leq 0 
	\end{cases}
\end{equation}
where $\alpha$ is now typically set to 1.0. ELU smooths the transition around $z = 0$ and helps with vanishing gradients while maintaining negative values to push mean activations closer to zero \parencite{clevert2015_elu}.

\paragraph{Logistic sigmoid}
The S-shaped sigmoid (also known as \emph{logit}) is given by:
\begin{equation}
	\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
\end{equation}
and its derivative reads
\begin{equation}
	\sigma'(z) = \sigma(z)\left(1 - \sigma(z)\right)
\end{equation}
This activation function shrinks the signal to the range $(0,1)$ \parencite{mira1995_sigmoid}. Historically, it has been used for binary classification. It mainly suffers from vanishing gradients for large values of $\abs{z}$ \parencite{glorot2010_trainingdiff}.

\paragraph{Hyperbolic tangent}
The hyperbolic tangent is given by:
\begin{equation}
	\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = \frac{e^{2z} - 1}{e^{2z} + 1}
\end{equation}
and its derivative reads
\begin{equation}
	\tanh'(z) = 1 - \tanh^2(z)
\end{equation}
This activation function squashes the signal to the range $(-1,1)$ \parencite{lecun1998_tanh}. It is zero-centered, which helps with gradient flow during training, but still suffers from vanishing gradients for large $\abs{z}$ \parencite{glorot2010_trainingdiff}.

\paragraph{Softmax}
The Softmax function for a vector $\bs{z} = (z_1, \dots, z_K)$ is defined as:
\begin{equation}
	\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\end{equation}
and its derivative for the Jacobian matrix elements is:
\begin{equation}
	\frac{\partial \text{Softmax}(z_i)}{\partial z_j} = \text{Softmax}(z_i)(\delta_{ij} - \text{Softmax}(z_j))
\end{equation}
where $\delta_{ij}$ is the Kronecker delta. Softmax converts the classical logit to a probability distribution and is primarily used in the output layer for multi-class classification \parencite{bridle1990_softmax}, as in our case, namely.

\begin{figure*}
	\centering
	\includegraphics[width=15cm]{ols_Vs_nn}
	\caption{\textbf{Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$.}
	\label{fig:nnols}
\end{figure*}

\subsection{Cost functions}
For this work, two different types of cost functions have been employed, depending on the type of model (simple regression or classification).
\paragraph{Mean Squared Error}
The Mean Squared Error (MSE) loss function for regression tasks is defined as:
\begin{equation}
	\text{MSE}(\bs{y}, \bs{\tilde{y}}, \bs{\Theta}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\end{equation}
with derivative:
\begin{equation}
\pdv{\text{MSE}}{\hat{y}_i} = \frac{2}{n} (y_i - \hat{y}_i)
\end{equation}
MSE is general convex and has smooth derivatives, therefore is usually a standard choice for machine learning problems that include gradient descent \parencite{hastie_stat2009}.

\paragraph{Cross-Entropy}
Cross-Entropy class functions are use for classification-oriented NNs, as they express the cost in term of a probability of belonging to a given class \parencite{bishop2006_pattern}.
The simplest case is given by the Binary Cross-Entropy (BCE):
\[
\begin{split}
	\text{BCE}(\bs{y}, \bs{\tilde{y}}, \bs{\Theta})=\\ = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i) \right]
\end{split}
\]
with derivative:
\begin{equation}
\pdv{\text{BCE}}{p_i}= \frac{p_i - y_i}{p_i(1 - p_i)}
\end{equation}
For multi-class models, the formulation is modified as follows:
\begin{equation}
	\text{CE} = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^K y_{ij} \log(\hat{y}_{ij} + \epsilon)
\end{equation}
with gradient:
\begin{equation}
	\pdv{\text{CE}}{\hat{y}_{ij}} = \frac{\hat{y}_{ij} - y_{ij}}{n}
\end{equation}
where $\epsilon$ is a small constant for numerical stability. Here, $K$ represents the total number of classes. In this case, furthermore, the output $\hat{y}_{ij}$ is \emph{one-hot encoded}. This type of encoding sets each class lab as a binary vector with values 1 for the true-class and 0 for the others \parencite{bishop2006_pattern, goodfellow_2016}.

\paragraph{Regularization}
The cost function can be regularized by applying a penalty factor, in order to perform a more generalized model. This can be done by using the Ridge and LASSO methods: the first adds an L2-type norm to the cost function, the second one an L1-type \parencite{goodfellow_2016}. Applying this penalties to the MSE leads to a shrinkage of the regression towards a constant model. Further details can be found in Project 1 report.

\subsection{Classification}
The classification effectiveness can be investigated through a confusion matrix. This is a widely used tool for evaluating the performance of classification models. It shows the relationships between predicted and true class labels in a tabular form, from which it is possible to compute scores such as accuracy and Cohen's $\kappa$ \parencite{powers2011evaluation, kohavi1998confusion}. Each cell in the matrix represents the number of instances belonging to a specific combination of predicted and actual classes.

The accuracy score is given by
\begin{equation}
\text{Accuracy} = \frac{\sum_{i=1}^n I(t_i = y_i)}{n} ,
\end{equation}
where $I$ is an indicator function, $1$ if $t_i = y_i$ and $0$
otherwise if we have a binary classification problem. Here $t_i$
represents the target and $y_i$ the outputs of our NN code and $n$ is simply the number of targets $t_i$.

\subsection{Set up of the code}
\subsubsection{Implementation}
For this project, a Python module has been created, containing all the necessary functions and classes for the analysis. The core is a Neural Network class, that can perform all the main algorithms (feedforward, backpropagation, training) and adapt to different types of input datasets and regression/classification frames.

Since we deal with learning machinery, Python libraries such as \texttt{scikit-learn} have also been employed, mainly for assessing the relative behavior of the own coded NN with respect to external modules.  More insights on this module's functionality can be found \href{https://scikit-learn.org/stable/user_guide.html}{here}.

The first part of the code is meant to implement all the activation and cost functions. The computation of the derivatives was approached in two different ways
\begin{itemize}
	\item analytical calculation;
	\item automatic differentiation with \texttt{autograd}
\end{itemize}

After that, an abstract class \texttt{Scheduler} has been created, in order to manage all the three different gradient descent algorithms (plain SGD, RMSProp and ADAM) independently of NN implementation.

The structure of \texttt{NeuralNetwork} class is shown in Algorithm \ref{alg:nn}
\begin{algorithm}
	\color{BlueViolet}
	\caption{Neural Network Class Structure}
	\begin{algorithmic}[1]
		\State \textbf{class} NeuralNetwork
			\State
			\Procedure{\_\_init\_\_}{} \EndProcedure
			\Procedure{\_create\_layers}{} \EndProcedure
			\Procedure{cost}{} \EndProcedure
			\Procedure{cost\_der}{} \EndProcedure
			\Procedure{\_feedforward}{} \EndProcedure
			\Procedure{\_backpropagate}{} \EndProcedure
			\Procedure{\_define\_scheduler}{} \EndProcedure
			\Procedure{\_update\_weights}{} \EndProcedure
			\Procedure{\_train}{} \EndProcedure
			\Procedure{\_predict}{} \EndProcedure
			\Procedure{\_accuracy}{} \EndProcedure
			\Procedure{\_reset\_weights}{} \EndProcedure
			\State
	\end{algorithmic}
	\label{alg:nn}
\end{algorithm}

\begin{figure*}[h]
	\centering
	\includegraphics[width=12cm]{skvsown}
	\caption{\textbf{Custom NN vs \sk} | Both algorithms produce similar results, though our NN is 29 times slower.}
	\label{fig:skvsown}
\end{figure*}
As said previously, it supports all the different types of cost and activation functions. Plus, an external function \texttt{set\_activations} allows choosing between automatic and manual differentiation. The cost functions and their respective derivatives can also be declared (MSE or Cross Entropy); their derivative are handled in the \texttt{cost\_der} function and it is again possible to choose between analytical solutions or \texttt{autograd}. If the chosen cost is the MSE, one can also decide to apply an L1 or L2 norm, by default set to 0.1.

The class also accept a flexible number of layers, each with a freely definable number of neurons.

\subsubsection{Testing}
Testing the custom NN was carried out in a Python Jupyter Notebook. The analysis consisted in:

\textbf{1. Loading data, preprocessing}\\
	A dataset based on Runge function was created, and the MNIST collection was imported from \texttt{scikit-learn}. 
	
	Both the datasets have been scaled; for the Runge dataset we employed \texttt{StandardScaler} from \texttt{scikit-learn.utils}. For the MNIST data, since we were dealing with pixel values in a gray-scale range [0, 255], we normalized all the pixel to the interval [0, 1], dividing by 255.
	
	Both have then been split in train and test sets, with a test size of 0.2.
	
\textbf{2. Comparing with linear OLS regression}\\
	The performance of the NN in a simple linear regression has been compared to the ordinary least squares code from Project 1. Two neural network objects have been created, one with one hidden layer of 50 nodes an the other with two hidden layers of 100 nodes. Only the sigmoid has been used as activation function.
	
\textbf{3. Testing different Stochastic Gradient Descent machineries}\\
	Plain SGD, RMSProp and ADAM have been tested on a range of different learning rates $\num{e-5}\leq\eta\leq\num{e-2}$ and for a number of epochs between \num{100} and \num{5000}. This step was important in the understanding of which algorithm could perform better and faster among the three.

\textbf{4. Comparing the own custom NN to \sk, \texttt{autograd} and \texttt{torch} routines}
The performance of our NN has been compared to the two machine learning modules, in order to be able to assess the effectiveness of our algorithm relatively to well known algorithms. We also ensured that the gradients calculated with \texttt{autograd} were the same as the manual computed ones.

\textbf{5. Applying L1 and L2 norms to MSE}
The effects of Ridge and LASSO regularizations have been explored for penalties $\num{e-5}\leq\lambda\leq1$ and for different learning rates.


\textbf{6. Classification analysis with MNIST}\\
The NN has been employed for a multi-class classification task. Its performance has been assessed with an accuracy score and a confusion matrix.

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{schedulers_mse_r2}
	\caption{\textbf{Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results with fewer epochs, showing robust performance across learning rates and iteration counts. In contrast, plain SGD performs better with lower rates and higher numbers of epochs.}
	\label{fig:sgdnn}
\end{figure*}

\section{Results}
\subsection{Comparison with linear OLS regression}
In Figure \ref{fig:nnols}, the results of a regression performed with both the \py{NeuralNetwork} and the \py{LinearRegression\_own} classes are shown. The latter was set with a polynomial degree of 13, chosen according to the Bias-Variance tradeoff results from analyses carried out in Project 1. 

In general, the best performance is achieved by the NN with two hidden layers, which shows the smallest deviation from the targets. Despite this, its $R^2$ is generally slightly lower than that of the linear OLS model.

\subsection{Testing of different Stochastic Gradient Descent solvers} \label{ss:sched}
First, the speed of each algorithm was tested by repeating 50 training runs with a NN consisting of 2 hidden layers of 100 nodes each, all using a sigmoid activation. For all algorithms, an initial learning rate of $\num{5e-3}$ and 500 epochs were set. For each iteration, execution time was recorded, and the average elapsed time over the 50 iterations was calculated for each scheduler, as shown in Table \ref{tab:schedtimes}. RMSProp was on average 1.4 times slower than plain SGD, and ADAM 1.7 times slower.
\begin{table}
	\centering
	\begin{tabular}{lSS}
		\toprule
		Scheduler & {Mean elapsed time} & {$\pm\,\sigma_{\text{mean}}$}\\
		& {(\unit{\second})} & {(\unit{\second})}\\
		\midrule
		Constant $\eta$ & 1.673 & 0.025 \\
		RMSProp & 2.379  &  0.020 \\
		ADAM & 2.811 & 0.032 \\
		\bottomrule
	\end{tabular}
	\caption{\textbf{Execution time for different SGD algorithms} | ADAM and RMSProp are slower than plain SGD. A standard deviation of the mean is also shown.}
	\label{tab:schedtimes}
\end{table}

Second, the overall effectiveness of the three schedulers was investigated as a function of learning rate and number of epochs. The results, shown in Figure \ref{fig:sgdnn}, indicate that RMSProp and ADAM, although slower, reach acceptable MSE and $R^2$ values even with relatively few epochs, \emph{i.e.}, they converge faster to an optimal solution.

\subsection{Custom NN and external libraries}
The speed and performance of our NN were compared with \texttt{MLPRegressor} from \sk. Using the same setup—two hidden layers with tanh activation, initial learning rate of \num{1e-3}, and small L2 regularization—the execution time difference was significant. Both used ADAM as optimizer. Our NN completed the task in $\sim \qty{11.5}{\second}$, whereas \sk completed it in $\sim \qty{0.4}{\second}$, making it 29 times faster. The resulting fits are similar, although our NN produced a slightly better approximation on the test set ($R_\text{own}^2 = 0.9995$, $R_\text{sk-learn}^2 = 0.9932$). The fits are shown in Figure \ref{fig:skvsown}.

We also confirmed that gradients computed with \texttt{autograd} (by setting \texttt{autodiff} to True) matched the manually computed ones. Two NNs with identical setups but different differentiation methods yielded identical fits.

\subsection{Adding L1 and L2 regularizations}
The results of the regularized fits are shown in Figure \ref{fig:nnl1l2}. Acceptable results were obtained for $\lambda\leq \num{e-3}$ and $\eta\leq \num{5e-5}$. The quality of MSE and $R^2$ does not decay gradually; instead, there is a sharp transition at these thresholds.
\begin{figure*}
	\centering
	\includegraphics[width=12cm]{nnl1l2}
	\caption{\textbf{L1 and L2 regularization} | Applying a penalty $\lambda \leq \num{e-3}$ can improve model generalization.}
	\label{fig:nnl1l2}
\end{figure*}

\subsection{Classification of MNIST data}
MNIST-784 was correctly loaded, scaled, and split into train/test sets. Four sample images are shown in Figure \ref{fig:mnistsamples}.
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{mnist_samples}
	\caption{\textbf{Samples from \href{https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784}} | Handwritten digits are 28$\times$28 pixels with gray scale values from 0 to 255.}
	\label{fig:mnistsamples}
\end{figure}

Classification was performed using a NN with two hidden layers of 128 and 64 nodes, respectively. Leaky ReLU was used for hidden layers, and Softmax for the output layer. ADAM was set as optimizer with 1000 epochs and learning rate \num{1e-3}, chosen according to previous results (Section \ref{ss:sched}).

Classification was successful, as shown in the confusion matrix in Figure \ref{fig:cm}. A high percentage of samples fall along the diagonal, indicating correct classification.
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{mnist_cm}
	\caption{\textbf{Confusion matrix of MNIST classification} | Overall accuracy of 0.92 indicates good performance.}
	\label{fig:cm}
\end{figure}

The classification accuracy was 0.92, which is considered very good.

\section{Discussion and conclusions}
The custom NN performed overall as expected. It was able to correctly approximate simple functions and classify handwritten digits.

When it comes to simple regressions, the choice of a linear regressor such as Ordinary Least Squares (OLS) remains a solid option due to its relatively simple implementation and fast execution. The equations that define the model are usually straightforward and are characterized by a limited number of parameters \parencite{wooditch2021_ols}. However, the main limitation of such methods is that they assume a linear relationship between the dependent and independent variables. For functions such as the Runge one, this can represent a problem, since a polynomial approximation will not be able to properly capture the nonlinearities of the relationship. NNs, thanks to the nonlinear activation functions they include, are potentially a better choice for this type of system \parencite{deridder1999_nn, waterworth2000_nn}. This can be directly observed in Figure \ref{fig:nnols}: even a complex polynomial OLS model cannot perform as well as a relatively simple two-hidden-layer NN, which correctly identifies the nonlinearities in the given function. Moreover, modern training techniques such as stochastic gradient descent and regularization strategies mitigate overfitting while maintaining model effectiveness \parencite{lecun1998_tanh}.

Choosing an NN over a linear regressor can still have drawbacks: it can take some time to find the best trade-off between complexity and efficiency, and very deep networks require a significant amount of computation time. In our case, the OLS regression fitted the Runge dataset in \qty{0.0011}{\second}, whereas the two-hidden-layer NN with 100 nodes and 1000 epochs required \qty{4.58}{\second}, which is about 4000 times slower.

Nevertheless, our results showed that highly efficient algorithms from external libraries are able to tackle the same tasks within a much smaller time interval, and this highlights one of the main weaknesses of our custom code: despite the overall consistent results, the code is still far from being considered fine-tuned.

For classification purposes, NNs are the only reasonable option, as linear regression assumes a continuous output space and does not constrain predictions to lie within class probability bounds (e.g., between 0 and 1). As a result, linear regression can yield invalid probability estimates and poorly defined decision boundaries \parencite{hosmer2013applied, bishop2006_pattern}. In our case, \ie a ten-class classification, NNs were able to correctly identify all patterns in the digits, resulting in high accuracy.

We can finally discuss the performance of our NN with regard to the choice of complexity, parameters, and optimization mechanisms.

In the context of this analysis, shallow networks with two or at most three layers and no more than 150 nodes per layer have proven to be a solid choice for tasks such as simple nonlinear regressions and the classification of very small images (on the order of \num{10}–\num{e2} squared pixels), even with a relatively large input dataset (\num{70000} for MNIST-784). Nevertheless, choosing a single hidden layer network can easily lead to excessive simplification and yield unacceptable results, as shown in Figure \ref{fig:nnols}.

The choice of activation functions is also crucial. We carried out several experiments to determine which functions could handle different problems most effectively. The sigmoid and tanh functions appear to offer a relatively good trade-off for regressions, particularly thanks to their nonlinearity. In contrast, the LeakyReLU and ReLU were not able to properly represent the curvature of the Runge function, resulting in an oversimplified approximation characterized by sharp corners. However, the ReLU family showed optimal results for discrete systems, such as classification. This can be explained by the fact that ReLU does not saturate for positive values and is also easier to compute \parencite{glorot2010_trainingdiff}.

Regarding optimization, plain SGD with a constant learning rate is a good option if the priority is a simple implementation coupled with fast execution for each epoch. However, adaptive learning rates represent a more robust choice that can yield better results almost \emph{independently} of the number of epochs and the initial learning rate value, according to our findings (Figure \ref{fig:sgdnn}). In fact, ADAM and RMSProp showed faster convergence, returning optimal weights and biases after only a relatively small number of epochs (250). Furthermore, these methods are generally more numerically stable, as they can produce acceptable results even for higher learning rates ($\eta \sim \num{e-3}$), overcoming the typical fixed-learning-rate divergence issues. These behaviors are also widely recognized in previous studies \parencite{tieleman2012rmsprop, kingma:adam, goodfellow_2016}.

The application of an L1 or L2 norm can also be considered and can certainly improve the model, particularly where a non-regularized network would fail or when working with a very small input dataset. However, our findings suggest avoiding penalties $\lambda \geq 0.01$, as this can lead to serious underfitting and loss of useful complexity. This is also confirmed in existing literature \parencite{ng2004feature}.

To conclude, we can state that the objectives of our analysis have been largely achieved. A general understanding of which hyperparameters, levels of complexity, and activation functions should be chosen depending on the system type is now provided. To evaluate more consistently how to tune such parameters, cross-validation and bias–variance trade-off analyses could be carried out, as previously done in Project 1. Finally, we conclude this work with a fairly low-level and non-optimized NN. Higher efficiency could certainly be reached by taking inspiration from well-performing modules such as \href{https://pytorch.org/}{\texttt{pytorch}} and \href{https://www.tensorflow.org/}{\texttt{tensorflow}}.

\newpage

\printbibliography[heading=bibintoc]
	
\end{document}