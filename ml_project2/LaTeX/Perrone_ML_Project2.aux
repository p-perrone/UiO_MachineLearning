\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{apa/global//global/global/global}
\HyPL@Entry{0<</S/D>>}
\babel@aux{french}{}
\babel@aux{english}{}
\HyPL@Entry{1<</S/D>>}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wang2003_nn}
\abx@aux@segm{0}{0}{wang2003_nn}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{haykin1994_ml}
\abx@aux@segm{0}{0}{haykin1994_ml}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{cybenko1989_approx}
\abx@aux@segm{0}{0}{cybenko1989_approx}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Neural Network} | A typical structure.}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn}{{1}{2}{\textbf {Neural Network} | A typical structure}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\abx@aux@page{1}{2}
\abx@aux@page{2}{2}
\abx@aux@page{3}{2}
\abx@aux@page{5}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Runge function} | In orange with an added noise of normal distribution $\mathcal  {N}$(0, 0.1). The $x$ values have already been scaled.}}{2}{figure.caption.3}\protected@file@percent }
\newlabel{fig:runge}{{2}{2}{\textbf {Runge function} | In orange with an added noise of normal distribution $\mathcal {N}$(0, 0.1). The $x$ values have already been scaled}{figure.caption.3}{}}
\abx@aux@page{6}{2}
\abx@aux@page{7}{2}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The universal approximation theorem}{2}{subsection.2.1}\protected@file@percent }
\abx@aux@page{8}{2}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hornik1991_approximation}
\abx@aux@segm{0}{0}{hornik1991_approximation}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\newlabel{th:theoexample}{{2.1}{3}{Universal approximation}{tcb@cnt@mytheo.2.1}{}}
\abx@aux@page{9}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Structure of a Neural Network}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}The Feed-forward algorithm}{3}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Simple perceptron model with 1-D input}{3}{subsubsection.2.2.1}\protected@file@percent }
\abx@aux@page{10}{3}
\@writefile{toc}{\contentsline {paragraph}{Generalization to a multi-layer network with multiple inputs}{3}{equation.5}\protected@file@percent }
\newlabel{eq:ff}{{6}{3}{Generalization to a multi-layer network with multiple inputs}{equation.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{3}{equation.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}The Backpropagation algorithm}{3}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Simple perceptron model} | In blue the different layers, in orange the functions.}}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:simpleperc}{{3}{4}{\textbf {Simple perceptron model} | In blue the different layers, in orange the functions}{figure.caption.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feedforward}}{4}{algorithm.1}\protected@file@percent }
\newlabel{alg:ff}{{1}{4}{Feedforward}{algorithm.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Formalism of backpropagation}{4}{equation.8}\protected@file@percent }
\newlabel{eq:deltaL}{{14}{4}{Formalism of backpropagation}{equation.14}{}}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hastie_stat2009}
\abx@aux@segm{0}{0}{hastie_stat2009}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{tieleman2012rmsprop}
\abx@aux@segm{0}{0}{tieleman2012rmsprop}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{kingma:adam}
\abx@aux@segm{0}{0}{kingma:adam}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{5}{figure.caption.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backpropagation}}{5}{algorithm.2}\protected@file@percent }
\newlabel{alg:bp}{{2}{5}{Backpropagation}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Training the Neural Network}{5}{subsubsection.2.2.3}\protected@file@percent }
\abx@aux@page{11}{5}
\abx@aux@page{12}{5}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent training}}{5}{algorithm.3}\protected@file@percent }
\abx@aux@page{13}{5}
\abx@aux@page{14}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Activation functions}{5}{subsection.2.3}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hochreiter1991_untersuchungen}
\abx@aux@segm{0}{0}{hochreiter1991_untersuchungen}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{philipp2018_explgrads}
\abx@aux@segm{0}{0}{philipp2018_explgrads}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{nair2010_relu}
\abx@aux@segm{0}{0}{nair2010_relu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{maas2013_leakyrelu}
\abx@aux@segm{0}{0}{maas2013_leakyrelu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{maas2013_leakyrelu}
\abx@aux@segm{0}{0}{maas2013_leakyrelu}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Activation functions} | A representation of each function.}}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:af}{{4}{6}{\textbf {Activation functions} | A representation of each function}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Activation functions and numerical instabilities}{6}{subsubsection.2.3.1}\protected@file@percent }
\abx@aux@page{15}{6}
\abx@aux@page{16}{6}
\abx@aux@page{17}{6}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Employed activation functions}{6}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Unit (ReLU)}{6}{subsubsection.2.3.2}\protected@file@percent }
\abx@aux@page{18}{6}
\abx@aux@page{19}{6}
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU}{6}{figure.caption.6}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{clevert2015_elu}
\abx@aux@segm{0}{0}{clevert2015_elu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{mira1995_sigmoid}
\abx@aux@segm{0}{0}{mira1995_sigmoid}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{lecun1998_tanh}
\abx@aux@segm{0}{0}{lecun1998_tanh}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bridle1990_softmax}
\abx@aux@segm{0}{0}{bridle1990_softmax}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$.}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:nnols}{{5}{7}{\textbf {Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$}{figure.caption.6}{}}
\abx@aux@page{20}{7}
\@writefile{toc}{\contentsline {paragraph}{Exponential Linear Unit (ELU)}{7}{equation.22}\protected@file@percent }
\abx@aux@page{21}{7}
\@writefile{toc}{\contentsline {paragraph}{Logistic sigmoid}{7}{equation.24}\protected@file@percent }
\abx@aux@page{22}{7}
\abx@aux@page{23}{7}
\@writefile{toc}{\contentsline {paragraph}{Hyperbolic tangent}{7}{equation.26}\protected@file@percent }
\abx@aux@page{24}{7}
\abx@aux@page{25}{7}
\@writefile{toc}{\contentsline {paragraph}{Softmax}{7}{equation.28}\protected@file@percent }
\abx@aux@page{26}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Cost functions}{7}{subsection.2.4}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hastie_stat2009}
\abx@aux@segm{0}{0}{hastie_stat2009}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results with fewer epochs, showing robust performance across learning rates and iteration counts. In contrast, plain SGD performs better with lower rates and higher numbers of epochs.}}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:sgdnn}{{6}{8}{\textbf {Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results with fewer epochs, showing robust performance across learning rates and iteration counts. In contrast, plain SGD performs better with lower rates and higher numbers of epochs}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Mean Squared Error}{8}{figure.caption.7}\protected@file@percent }
\abx@aux@page{27}{8}
\@writefile{toc}{\contentsline {paragraph}{Cross-Entropy}{8}{equation.32}\protected@file@percent }
\abx@aux@page{28}{8}
\abx@aux@page{29}{8}
\abx@aux@page{30}{8}
\@writefile{toc}{\contentsline {paragraph}{Regularization}{8}{equation.35}\protected@file@percent }
\abx@aux@page{31}{8}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{powers2011evaluation}
\abx@aux@segm{0}{0}{powers2011evaluation}
\abx@aux@cite{0}{kohavi1998confusion}
\abx@aux@segm{0}{0}{kohavi1998confusion}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Classification}{9}{subsection.2.5}\protected@file@percent }
\abx@aux@page{32}{9}
\abx@aux@page{33}{9}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Set up of the code}{9}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Implementation}{9}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Neural Network Class Structure}}{9}{algorithm.4}\protected@file@percent }
\newlabel{alg:nn}{{4}{9}{Neural Network Class Structure}{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Testing}{9}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Custom NN vs \tcbox [boxsep=1pt, left=2pt, right=2pt]{\texttt  {scikit-learn}}} | Both algorithms produce similar results, though our NN is 29 times slower.}}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:skvsown}{{7}{10}{\textbf {Custom NN vs \sk } | Both algorithms produce similar results, though our NN is 29 times slower}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{10}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Comparison with linear OLS regression}{10}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Execution time for different SGD algorithms} | ADAM and RMSProp are slower than plain SGD. A standard deviation of the mean is also shown.}}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:schedtimes}{{1}{10}{\textbf {Execution time for different SGD algorithms} | ADAM and RMSProp are slower than plain SGD. A standard deviation of the mean is also shown}{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Testing of different Stochastic Gradient Descent solvers}{10}{subsection.3.2}\protected@file@percent }
\newlabel{ss:sched}{{3.2}{10}{Testing of different Stochastic Gradient Descent solvers}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Custom NN and external libraries}{10}{subsection.3.3}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wooditch2021_ols}
\abx@aux@segm{0}{0}{wooditch2021_ols}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{deridder1999_nn}
\abx@aux@segm{0}{0}{deridder1999_nn}
\abx@aux@cite{0}{waterworth2000_nn}
\abx@aux@segm{0}{0}{waterworth2000_nn}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{lecun1998_tanh}
\abx@aux@segm{0}{0}{lecun1998_tanh}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {L1 and L2 regularization} | Applying a penalty $\lambda \leq \num {e-3}$ can improve model generalization.}}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:nnl1l2}{{8}{11}{\textbf {L1 and L2 regularization} | Applying a penalty $\lambda \leq \num {e-3}$ can improve model generalization}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Adding L1 and L2 regularizations}{11}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification of MNIST data}{11}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion and conclusions}{11}{section.4}\protected@file@percent }
\abx@aux@page{34}{11}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hosmer2013applied}
\abx@aux@segm{0}{0}{hosmer2013applied}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{tieleman2012rmsprop}
\abx@aux@segm{0}{0}{tieleman2012rmsprop}
\abx@aux@cite{0}{kingma:adam}
\abx@aux@segm{0}{0}{kingma:adam}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Samples from \href  {https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784}} | Handwritten digits are 28$\times $28 pixels with gray scale values from 0 to 255.}}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:mnistsamples}{{9}{12}{\textbf {Samples from \href {https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784}} | Handwritten digits are 28$\times $28 pixels with gray scale values from 0 to 255}{figure.caption.11}{}}
\abx@aux@page{35}{12}
\abx@aux@page{36}{12}
\abx@aux@page{37}{12}
\abx@aux@page{38}{12}
\abx@aux@page{39}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Confusion matrix of MNIST classification} | Overall accuracy of 0.92 indicates good performance.}}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:cm}{{10}{12}{\textbf {Confusion matrix of MNIST classification} | Overall accuracy of 0.92 indicates good performance}{figure.caption.12}{}}
\abx@aux@page{40}{12}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{ng2004feature}
\abx@aux@segm{0}{0}{ng2004feature}
\abx@aux@page{41}{13}
\abx@aux@page{42}{13}
\abx@aux@page{43}{13}
\abx@aux@page{44}{13}
\@writefile{toc}{\contentsline {section}{References}{13}{figure.caption.12}\protected@file@percent }
\abx@aux@page{45}{13}
\abx@aux@page{46}{13}
\abx@aux@page{47}{13}
\abx@aux@page{48}{13}
\abx@aux@page{49}{13}
\abx@aux@page{50}{13}
\abx@aux@page{51}{13}
\abx@aux@page{52}{13}
\abx@aux@page{53}{13}
\abx@aux@page{54}{13}
\abx@aux@page{55}{13}
\abx@aux@page{56}{13}
\abx@aux@page{57}{13}
\abx@aux@page{58}{13}
\abx@aux@page{59}{13}
\abx@aux@page{60}{13}
\abx@aux@page{61}{13}
\abx@aux@page{62}{13}
\abx@aux@page{63}{13}
\abx@aux@page{64}{13}
\abx@aux@page{65}{13}
\abx@aux@page{66}{14}
\abx@aux@page{67}{14}
\abx@aux@page{68}{14}
\abx@aux@page{69}{14}
\abx@aux@page{70}{14}
\abx@aux@read@bbl@mdfivesum{73F4B33DA4AD48BC304375D13F7E748A}
\abx@aux@read@bblrerun
\abx@aux@defaultrefcontext{0}{bishop2006_pattern}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bridle1990_softmax}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2015_elu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cybenko1989_approx}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{deridder1999_nn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{glorot2010_trainingdiff}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow_2016}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastie_stat2009}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{haykin1994_ml}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1991_untersuchungen}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornik1991_approximation}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hosmer2013applied}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma:adam}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kohavi1998confusion}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun1998_tanh}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maas2013_leakyrelu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mira1995_sigmoid}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nair2010_relu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ng2004feature}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{philipp2018_explgrads}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{powers2011evaluation}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblatt1958_perceptron}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tieleman2012rmsprop}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2003_nn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{waterworth2000_nn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wooditch2021_ols}{apa/global//global/global/global}
\gdef \@abspage@last{15}
