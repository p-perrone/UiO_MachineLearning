\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{apa/global//global/global/global}
\HyPL@Entry{0<</S/D>>}
\babel@aux{french}{}
\babel@aux{english}{}
\HyPL@Entry{1<</S/D>>}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wang2003_nn}
\abx@aux@segm{0}{0}{wang2003_nn}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{haykin1994_ml}
\abx@aux@segm{0}{0}{haykin1994_ml}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{cybenko1989_approx}
\abx@aux@segm{0}{0}{cybenko1989_approx}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hornik1991_approximation}
\abx@aux@segm{0}{0}{hornik1991_approximation}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\abx@aux@page{1}{2}
\abx@aux@page{2}{2}
\abx@aux@page{3}{2}
\abx@aux@page{4}{2}
\abx@aux@page{5}{2}
\abx@aux@page{6}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Neural Network} | A typical structure.}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn}{{1}{2}{\textbf {Neural Network} | A typical structure}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The universal approximation theorem}{2}{subsection.2.1}\protected@file@percent }
\abx@aux@page{7}{2}
\newlabel{th:theoexample}{{2.1}{2}{Universal approximation}{tcb@cnt@mytheo.2.1}{}}
\abx@aux@page{8}{2}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Structure of a Neural Network}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}The Feed-forward algorithm}{3}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Simple perceptron model with 1-D input}{3}{subsubsection.2.2.1}\protected@file@percent }
\abx@aux@page{9}{3}
\@writefile{toc}{\contentsline {paragraph}{Generalization to a multi-layer network with multiple inputs}{3}{equation.5}\protected@file@percent }
\newlabel{eq:ff}{{6}{3}{Generalization to a multi-layer network with multiple inputs}{equation.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{3}{equation.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feedforward}}{3}{algorithm.1}\protected@file@percent }
\newlabel{alg:ff}{{1}{3}{Feedforward}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}The Backpropagation algorithm}{3}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Simple perceptron model} | In blue the different layers, in orange the functions.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:simpleperc}{{2}{4}{\textbf {Simple perceptron model} | In blue the different layers, in orange the functions}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Formalism of backpropagation}{4}{equation.8}\protected@file@percent }
\newlabel{eq:deltaL}{{14}{4}{Formalism of backpropagation}{equation.14}{}}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hastie_stat2009}
\abx@aux@segm{0}{0}{hastie_stat2009}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{tieleman2012rmsprop}
\abx@aux@segm{0}{0}{tieleman2012rmsprop}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{kingma:adam}
\abx@aux@segm{0}{0}{kingma:adam}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hochreiter1991_untersuchungen}
\abx@aux@segm{0}{0}{hochreiter1991_untersuchungen}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{philipp2018_explgrads}
\abx@aux@segm{0}{0}{philipp2018_explgrads}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{5}{equation.18}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backpropagation}}{5}{algorithm.2}\protected@file@percent }
\newlabel{alg:bp}{{2}{5}{Backpropagation}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Training the Neural Network}{5}{subsubsection.2.2.3}\protected@file@percent }
\abx@aux@page{10}{5}
\abx@aux@page{11}{5}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent training}}{5}{algorithm.3}\protected@file@percent }
\abx@aux@page{12}{5}
\abx@aux@page{13}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Activation functions}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Activation functions and numerical instabilities}{5}{subsubsection.2.3.1}\protected@file@percent }
\abx@aux@page{14}{5}
\abx@aux@page{15}{5}
\abx@aux@page{16}{5}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{nair2010_relu}
\abx@aux@segm{0}{0}{nair2010_relu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{maas2013_leakyrelu}
\abx@aux@segm{0}{0}{maas2013_leakyrelu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{maas2013_leakyrelu}
\abx@aux@segm{0}{0}{maas2013_leakyrelu}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Activation functions} | A representation of each function}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:af}{{3}{6}{\textbf {Activation functions} | A representation of each function}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Employed activation functions}{6}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Unit (ReLU)}{6}{subsubsection.2.3.2}\protected@file@percent }
\abx@aux@page{17}{6}
\abx@aux@page{18}{6}
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU}{6}{equation.20}\protected@file@percent }
\abx@aux@page{19}{6}
\@writefile{toc}{\contentsline {paragraph}{Exponential Linear Unit (ELU)}{6}{equation.22}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{clevert2015_elu}
\abx@aux@segm{0}{0}{clevert2015_elu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{mira1995_sigmoid}
\abx@aux@segm{0}{0}{mira1995_sigmoid}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{lecun1998_tanh}
\abx@aux@segm{0}{0}{lecun1998_tanh}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bridle1990_softmax}
\abx@aux@segm{0}{0}{bridle1990_softmax}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hastie_stat2009}
\abx@aux@segm{0}{0}{hastie_stat2009}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@page{20}{7}
\@writefile{toc}{\contentsline {paragraph}{Logistic sigmoid}{7}{equation.24}\protected@file@percent }
\abx@aux@page{21}{7}
\abx@aux@page{22}{7}
\@writefile{toc}{\contentsline {paragraph}{Hyperbolic tangent}{7}{equation.26}\protected@file@percent }
\abx@aux@page{23}{7}
\abx@aux@page{24}{7}
\@writefile{toc}{\contentsline {paragraph}{Softmax}{7}{equation.28}\protected@file@percent }
\abx@aux@page{25}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Cost functions}{7}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean Squared Error}{7}{subsection.2.4}\protected@file@percent }
\abx@aux@page{26}{7}
\@writefile{toc}{\contentsline {paragraph}{Cross-Entropy}{7}{equation.32}\protected@file@percent }
\abx@aux@page{27}{7}
\abx@aux@page{28}{7}
\abx@aux@page{29}{7}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{powers2011evaluation}
\abx@aux@segm{0}{0}{powers2011evaluation}
\abx@aux@cite{0}{kohavi1998confusion}
\abx@aux@segm{0}{0}{kohavi1998confusion}
\@writefile{toc}{\contentsline {paragraph}{Regularization}{8}{equation.35}\protected@file@percent }
\abx@aux@page{30}{8}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Classification}{8}{subsection.2.5}\protected@file@percent }
\abx@aux@page{31}{8}
\abx@aux@page{32}{8}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Set up of the code}{8}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Implementation}{8}{subsubsection.2.6.1}\protected@file@percent }
\newlabel{alg:nn}{{\caption@xref {alg:nn}{ on input line 622}}{8}{Implementation}{subsubsection.2.6.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Neural Network Class Structure}}{8}{algorithm.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Testing}{8}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$.}}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nnols}{{4}{9}{\textbf {Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{9}{section.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Different execution time for each SGD algorithm} | ADAM and RMSProp are slower than the plain SGD. A standard deviation of the mean is also shown.}}{9}{table.caption.6}\protected@file@percent }
\newlabel{tab:schedtimes}{{1}{9}{\textbf {Different execution time for each SGD algorithm} | ADAM and RMSProp are slower than the plain SGD. A standard deviation of the mean is also shown}{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Comparison with linear OLS regression}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Testing of different Stochastic Gradient Descent solvers}{9}{subsection.3.2}\protected@file@percent }
\newlabel{ss:sched}{{3.2}{9}{Testing of different Stochastic Gradient Descent solvers}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results within a smaller amount of epochs, proving a fairly consistent behavior to any learning rate and number of iterations. On the opposite, plain SGD with constant learning rate seems performing better with lower rates and a higher number of epochs.}}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:sgdnn}{{5}{10}{\textbf {Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results within a smaller amount of epochs, proving a fairly consistent behavior to any learning rate and number of iterations. On the opposite, plain SGD with constant learning rate seems performing better with lower rates and a higher number of epochs}{figure.caption.7}{}}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wooditch2021_ols}
\abx@aux@segm{0}{0}{wooditch2021_ols}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{deridder1999_nn}
\abx@aux@segm{0}{0}{deridder1999_nn}
\abx@aux@cite{0}{waterworth2000_nn}
\abx@aux@segm{0}{0}{waterworth2000_nn}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{lecun1998_tanh}
\abx@aux@segm{0}{0}{lecun1998_tanh}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hosmer2013applied}
\abx@aux@segm{0}{0}{hosmer2013applied}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Custom NN and external libraries}{11}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Adding L1 and L2 regularizations}{11}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification of MNIST data}{11}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{11}{section.4}\protected@file@percent }
\abx@aux@page{33}{11}
\abx@aux@page{34}{11}
\abx@aux@page{35}{11}
\abx@aux@page{36}{11}
\abx@aux@page{37}{11}
\abx@aux@page{38}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Custom NN vs \tcbox [boxsep=1pt, left=2pt, right=2pt]{\texttt  {scikit-learn}}} | The two algorithms yield a similar result, even though our NN is 29 times slower.}}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:skvsown}{{6}{12}{\textbf {Custom NN vs \sk } | The two algorithms yield a similar result, even though our NN is 29 times slower}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Custom NN vs \tcbox [boxsep=1pt, left=2pt, right=2pt]{\texttt  {scikit-learn}}} | The two algorithms yield a similar result, even though our NN is 29 times slower.}}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:nnl1l2}{{7}{12}{\textbf {Custom NN vs \sk } | The two algorithms yield a similar result, even though our NN is 29 times slower}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Samples from \href  {https://www.kaggle.com/datasets/hojjatk/mnist-dataset}{MNIST-784} collection} | The hand-written digits are 28$\times $28 pixel sized and each pixel has gray scale value ranging from 0 to 255.}}{13}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Confusion matrix of MNIST data classification} | Good result, with overall accuracy of 0.92.}}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig:cm}{{9}{13}{\textbf {Confusion matrix of MNIST data classification} | Good result, with overall accuracy of 0.92}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{13}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{13}{section.5}\protected@file@percent }
\abx@aux@page{39}{13}
\abx@aux@page{40}{13}
\abx@aux@page{41}{13}
\abx@aux@page{42}{13}
\abx@aux@page{43}{13}
\abx@aux@page{44}{13}
\abx@aux@page{45}{13}
\abx@aux@page{46}{13}
\abx@aux@page{47}{13}
\abx@aux@page{48}{13}
\abx@aux@page{49}{13}
\abx@aux@page{50}{13}
\abx@aux@page{51}{14}
\abx@aux@page{52}{14}
\abx@aux@page{53}{14}
\abx@aux@page{54}{14}
\abx@aux@page{55}{14}
\abx@aux@page{56}{14}
\abx@aux@page{57}{14}
\abx@aux@page{58}{14}
\abx@aux@page{59}{14}
\abx@aux@page{60}{14}
\abx@aux@page{61}{14}
\abx@aux@page{62}{14}
\abx@aux@page{63}{14}
\abx@aux@read@bbl@mdfivesum{71057EC18ADD18E0B88C77B27EA8FE0F}
\abx@aux@defaultrefcontext{0}{bishop2006_pattern}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bridle1990_softmax}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2015_elu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cybenko1989_approx}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{deridder1999_nn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{glorot2010_trainingdiff}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow_2016}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastie_stat2009}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{haykin1994_ml}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1991_untersuchungen}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornik1991_approximation}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hosmer2013applied}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma:adam}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kohavi1998confusion}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun1998_tanh}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maas2013_leakyrelu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mira1995_sigmoid}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nair2010_relu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{philipp2018_explgrads}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{powers2011evaluation}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblatt1958_perceptron}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tieleman2012rmsprop}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2003_nn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{waterworth2000_nn}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wooditch2021_ols}{apa/global//global/global/global}
\gdef \@abspage@last{15}
