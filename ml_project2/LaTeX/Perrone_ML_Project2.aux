\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{apa/global//global/global/global}
\HyPL@Entry{0<</S/D>>}
\babel@aux{french}{}
\babel@aux{english}{}
\HyPL@Entry{1<</S/D>>}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wang2003_nn}
\abx@aux@segm{0}{0}{wang2003_nn}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{haykin1994_ml}
\abx@aux@segm{0}{0}{haykin1994_ml}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{cybenko1989_approx}
\abx@aux@segm{0}{0}{cybenko1989_approx}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hornik1991_approximation}
\abx@aux@segm{0}{0}{hornik1991_approximation}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\abx@aux@page{1}{2}
\abx@aux@page{2}{2}
\abx@aux@page{3}{2}
\abx@aux@page{4}{2}
\abx@aux@page{5}{2}
\abx@aux@page{6}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Neural Network} | A typical structure.}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn}{{1}{2}{\textbf {Neural Network} | A typical structure}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The universal approximation theorem}{2}{subsection.2.1}\protected@file@percent }
\abx@aux@page{7}{2}
\newlabel{th:theoexample}{{2.1}{2}{Universal approximation}{tcb@cnt@mytheo.2.1}{}}
\abx@aux@page{8}{2}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Structure of a Neural Network}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}The Feed-forward algorithm}{3}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Simple perceptron model with 1-D input}{3}{subsubsection.2.2.1}\protected@file@percent }
\abx@aux@page{9}{3}
\@writefile{toc}{\contentsline {paragraph}{Generalization to a multi-layer network with multiple inputs}{3}{equation.5}\protected@file@percent }
\newlabel{eq:ff}{{6}{3}{Generalization to a multi-layer network with multiple inputs}{equation.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{3}{equation.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feedforward}}{3}{algorithm.1}\protected@file@percent }
\newlabel{alg:ff}{{1}{3}{Feedforward}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}The Backpropagation algorithm}{3}{subsubsection.2.2.2}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hastie_stat2009}
\abx@aux@segm{0}{0}{hastie_stat2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Simple perceptron model} | In blue the different layers, in orange the functions.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:simpleperc}{{2}{4}{\textbf {Simple perceptron model} | In blue the different layers, in orange the functions}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Formalism of backpropagation}{4}{equation.8}\protected@file@percent }
\newlabel{eq:deltaL}{{14}{4}{Formalism of backpropagation}{equation.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{4}{equation.18}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{tieleman2012rmsprop}
\abx@aux@segm{0}{0}{tieleman2012rmsprop}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{kingma:adam}
\abx@aux@segm{0}{0}{kingma:adam}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hochreiter1991_untersuchungen}
\abx@aux@segm{0}{0}{hochreiter1991_untersuchungen}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{philipp2018_explgrads}
\abx@aux@segm{0}{0}{philipp2018_explgrads}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{nair2010_relu}
\abx@aux@segm{0}{0}{nair2010_relu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{maas2013_leakyrelu}
\abx@aux@segm{0}{0}{maas2013_leakyrelu}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backpropagation}}{5}{algorithm.2}\protected@file@percent }
\newlabel{alg:bp}{{2}{5}{Backpropagation}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Training the Neural Network}{5}{subsubsection.2.2.3}\protected@file@percent }
\abx@aux@page{10}{5}
\abx@aux@page{11}{5}
\abx@aux@page{12}{5}
\abx@aux@page{13}{5}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent training}}{5}{algorithm.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Activation functions}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Activation functions and numerical instabilities}{5}{subsubsection.2.3.1}\protected@file@percent }
\abx@aux@page{14}{5}
\abx@aux@page{15}{5}
\abx@aux@page{16}{5}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Employed activation functions}{5}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Unit (ReLU)}{5}{subsubsection.2.3.2}\protected@file@percent }
\abx@aux@page{17}{5}
\abx@aux@page{18}{5}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{maas2013_leakyrelu}
\abx@aux@segm{0}{0}{maas2013_leakyrelu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{clevert2015_elu}
\abx@aux@segm{0}{0}{clevert2015_elu}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{mira1995_sigmoid}
\abx@aux@segm{0}{0}{mira1995_sigmoid}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Activation functions} | A representation of each function}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:af}{{3}{6}{\textbf {Activation functions} | A representation of each function}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU}{6}{equation.20}\protected@file@percent }
\abx@aux@page{19}{6}
\@writefile{toc}{\contentsline {paragraph}{Exponential Linear Unit (ELU)}{6}{equation.22}\protected@file@percent }
\abx@aux@page{20}{6}
\@writefile{toc}{\contentsline {paragraph}{Logistic sigmoid}{6}{equation.24}\protected@file@percent }
\abx@aux@page{21}{6}
\abx@aux@page{22}{6}
\@writefile{toc}{\contentsline {paragraph}{Hyperbolic tangent}{6}{equation.26}\protected@file@percent }
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{lecun1998_tanh}
\abx@aux@segm{0}{0}{lecun1998_tanh}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{glorot2010_trainingdiff}
\abx@aux@segm{0}{0}{glorot2010_trainingdiff}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bridle1990_softmax}
\abx@aux@segm{0}{0}{bridle1990_softmax}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hastie_stat2009}
\abx@aux@segm{0}{0}{hastie_stat2009}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bishop2006_pattern}
\abx@aux@segm{0}{0}{bishop2006_pattern}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{goodfellow_2016}
\abx@aux@segm{0}{0}{goodfellow_2016}
\abx@aux@page{23}{7}
\abx@aux@page{24}{7}
\@writefile{toc}{\contentsline {paragraph}{Softmax}{7}{equation.28}\protected@file@percent }
\abx@aux@page{25}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Cost functions}{7}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean Squared Error}{7}{subsection.2.4}\protected@file@percent }
\abx@aux@page{26}{7}
\@writefile{toc}{\contentsline {paragraph}{Cross-Entropy}{7}{equation.32}\protected@file@percent }
\abx@aux@page{27}{7}
\abx@aux@page{28}{7}
\abx@aux@page{29}{7}
\@writefile{toc}{\contentsline {paragraph}{Regularization}{7}{equation.35}\protected@file@percent }
\abx@aux@page{30}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Set up of the code}{7}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Implementation}{7}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Testing}{8}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Different execution time for each SGD algorithm} | ADAM and RMSProp are slower than the plain SGD. A standard deviation of the mean is also shown.}}{8}{table.caption.6}\protected@file@percent }
\newlabel{tab:schedtimes}{{1}{8}{\textbf {Different execution time for each SGD algorithm} | ADAM and RMSProp are slower than the plain SGD. A standard deviation of the mean is also shown}{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Comparison with linear OLS regression}{8}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Testing of different Stochastic Gradient Descent solvers}{8}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}}{8}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$.}}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nnols}{{4}{9}{\textbf {Linear OLS and Neural Networks} | Three different models have been tested for a simple regression. A linear ordinary least squares model with degree 13 showed good test MSE and $R^2$. The NN with two hidden layers of 100 nodes performed even better with an MSE 50 times smaller than the previous one. The NN with one hidden layer only shows serious underfitting and it is not able to properly represent the function, with a strongly negative $R^2$}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results within a smaller amount of epochs, proving a fairly consistent behavior to any learning rate and number of iterations. On the opposite, plain SGD with constant learning rate seems performing better with lower rates and a higher number of epochs.}}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:sgdnn}{{5}{9}{\textbf {Performance of different SGD solvers} | RMSProp and ADAM converge to acceptable results within a smaller amount of epochs, proving a fairly consistent behavior to any learning rate and number of iterations. On the opposite, plain SGD with constant learning rate seems performing better with lower rates and a higher number of epochs}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{10}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{10}{section.5}\protected@file@percent }
\abx@aux@page{31}{10}
\abx@aux@page{32}{10}
\abx@aux@page{33}{10}
\abx@aux@page{34}{10}
\abx@aux@page{35}{10}
\abx@aux@page{36}{10}
\abx@aux@page{37}{10}
\abx@aux@page{38}{10}
\abx@aux@page{39}{10}
\abx@aux@page{40}{10}
\abx@aux@page{41}{10}
\abx@aux@page{42}{10}
\abx@aux@page{43}{10}
\abx@aux@page{44}{10}
\abx@aux@page{45}{10}
\abx@aux@page{46}{10}
\abx@aux@page{47}{10}
\abx@aux@page{48}{10}
\abx@aux@page{49}{10}
\abx@aux@read@bbl@mdfivesum{992381C33AFC7FB45E4573F41905FC96}
\abx@aux@defaultrefcontext{0}{bishop2006_pattern}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bridle1990_softmax}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2015_elu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cybenko1989_approx}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{glorot2010_trainingdiff}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow_2016}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastie_stat2009}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{haykin1994_ml}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1991_untersuchungen}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornik1991_approximation}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma:adam}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun1998_tanh}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maas2013_leakyrelu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mira1995_sigmoid}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nair2010_relu}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{philipp2018_explgrads}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblatt1958_perceptron}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tieleman2012rmsprop}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2003_nn}{apa/global//global/global/global}
\gdef \@abspage@last{11}
